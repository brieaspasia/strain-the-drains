---
title: "Results-Figs"
author: "Brie Sherow"
date: "05/04/2021"
output: 
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    df_print: paged
  pdf_document:
    toc: yes
---

```{r setup, warning=FALSE, message=FALSE, results='hide', include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, warning=FALSE, message=FALSE, results='hide', include=FALSE}
library(ggplot2) #graphing
library(ggthemes) #graphing templates
library(hrbrthemes) #graphing templates
library(lubridate) #date manipulation
library(forcats) #working with factors
library(tidyverse) #manipulating data
library(knitr) #rmarkdown functions
library(tidyr) #long & wide formats
library(reshape2) #melt function for wide and long formats
library(stats) #R stats functions
library(broom) #create summaries from stats objects
library(car) #lm regression
library(MASS) #stats
library(lme4) #glmer function
library(DHARMa) #testing model diagnostics
library(glmmTMB) #fit zero-inflated negative binomial
# library(lattice) #fourth corner heatmap
# library(corrplot) #co-occurrence matrix
library(gclus) #co-occurrence matrix
library(broom.mixed) #regression tables
library(vegan) #ordination
library(emmeans)
library(ComplexHeatmap)
library(RColorBrewer)
library(cowplot) #minimal backgrounds for ggplot
library(purrr) #looping
```

```{r create-df, warning=FALSE, message=FALSE, results='hide', include=FALSE}
#create survey count  

        #load item counts
        item <- read.csv(file="Data/2101_item.csv", 
                         header=T, sep=",") 

#remove duplicate entries
        item <- unique(item[!duplicated(item),])

        #load item labels
        item_label <- read.csv(file="Data/item_label.csv", 
                         header=T, sep=",") 
        
        #load events
        event <- read.csv(file="Data/2101_event.csv", 
                         header=T, sep=",") 
        
        #join event info to item count
        survey_count <- left_join(item, event, by="event_id")
        
        survey_count <- survey_count %>%
          dplyr::select(-event.tot, -event.wt, -vol, -hr, -event_date, -note) %>%
          filter(item!="Pollution Rating") 

        #summarise duplicate survey entries (this will lose one set of notes....)
        item_merge <- survey_count %>%
          group_by(event_id, item) %>%
          summarise(sum=sum(total)) %>%
         ungroup()

        #join survey count to the merged summaries
        survey_count <- item_merge %>%
          left_join(survey_count, by=c("event_id"="event_id", "item"="item")) %>%
          dplyr::select(-total) %>%
          unique()
        
#create abundance data with all possible debris items and zero values

      # #count of items per unique event
      # event_count <- survey_count %>%
      #   group_by(event_id, item) %>%
      #   summarise(sum=sum(sum)) %>%
      #   ungroup()

      #load AMDI code\
      AMDI_code <- read.csv(file="Data/200101_AMDI_code.csv", 
                       header=T, sep=",") 
      
      #create full list of possible items
      AMDI <- AMDI_code %>%
        filter(item_code_id != "LSTD1") %>% #remove pollution rating / microplastics
        dplyr::select(item.ex, mat) #column of all possible item types
      
      
      #create a df of all possible items at all survey events, complete with zeros
      abund <- full_join(AMDI, survey_count, by=c("item.ex" = "item"))
      
      #data long to wide
      abund_wide <- abund %>%
        spread(item.ex, sum) %>%
        dplyr::select(-Asset.ID, -Cycle, -covid, -date, -mat)
      
      #replace na values with 0
      abund_wide[is.na(abund_wide)] <- 0
        
      #transform back to long but now with the full items list and zero data
      abund_long <- melt(abund_wide, id.vars="event_id")
      
      #remove the zero event ID (how did that get there?)
      abund_long <- abund_long[abund_long$event_id != 0, ]

#join abundance data to survey notes and other columns
      df <- left_join(abund_long, event, by="event_id")
      
        #load sites
          site <- read.csv(file="Data/200619_site.csv", 
                           header=T, sep=",") 
      
            #join LGA and luz to df
          df <- left_join(df, site, by = "Asset.ID")
          
          #classify items correctly      
           df <- df %>%
        dplyr::select(-event_date, -event.tot, -event.wt, -vol, -hr, 
                      -Site, -Latitude, -Longitude) %>% #remove duplicate columns
        rename(item=variable, #rename columns
               sum=value,
               asset_id = Asset.ID,
               cycle=Cycle,
               LUZ = Land.use.zone) %>% 
        mutate(asset_id=as.factor(asset_id), #classify columns correctly  
             item=as.factor(item),
             LGA=as.factor(LGA),
             LUZ=as.factor(LUZ),
             event_id=as.factor(event_id),
             cycle=as.factor(cycle),
             date=dmy(date)
             )

#resolve duplicates in MC01, C5    
          dup <- df %>% 
            group_by(asset_id, cycle, item) %>% #reconcile duplicates in MC01 Cycle 5
            summarise(sum=sum(sum)) %>%
            ungroup()
           
      # #check difference
      #      subsetdf <- df %>%
      #        dplyr::select(Asset.ID, Cycle, item, sum)
      #      
      #      setdiff(subsetdf, dup)
           
      #join C5 correction to df
      df <- dup %>% left_join(df, by=c("asset_id"="asset_id", 
                                       "cycle"="cycle", "item"="item")) %>%
        dplyr::select(-sum.y) %>%
        filter(event_id != "23097") %>%
        rename(sum = sum.x) %>%
        distinct()
      
      item_labelDF <- df %>%
        distinct(item)
      
      df <- df %>%
        left_join(item_label, by=c("item" = "item.type")) %>%
        mutate(item = A3) %>%
        dplyr::select(-A3)
      
#classify columns correctly    
      df <- df %>%
      mutate(item=as.factor(item),
             covid=as.factor(covid),
             material=as.factor(material))

#Combine C8 and C9    
     #isolate C8
    C8 <- df %>%
      filter(cycle == "C8")
    
    #isolate C9
    C9 <- df %>%
      filter(cycle == "C9")
    
    #join C8 and C9 by unique drain and item to capture double surveys
    t <- left_join(C9, C8, by=c("asset_id"="asset_id", "item" = "item"))
    t$sum.y[is.na(t$sum.y)] <- 0 #change na to 0
    
    t <- t %>%
      mutate(combined_sum = sum.y + sum.x) %>% #add C8 and C9 together
      dplyr::select(event_id.x, item, combined_sum)
    
    df <- df %>% 
      left_join(t, by = c("event_id"="event_id.x", "item"="item")) %>%
      filter(cycle != "C8") %>%
      mutate(combined_sum = replace_na(combined_sum, 0),
              sum2 = ifelse(cycle != "C9", sum, 0),
              final_sum = sum2 + combined_sum) %>%
      dplyr::select(-sum2, -sum, -combined_sum) %>%
      rename(sum = final_sum)
    
#find days since last survey at each asset
diff <- df %>%
  distinct(event_id, .keep_all = T) %>%
  dplyr::select(-item,-sum) %>%
  group_by(asset_id) %>% #looking at each event
  arrange(date) %>%
  mutate(days = date - lag(date)) %>% #days since last survey event
  filter(days != "NA") %>% #remove first survey date
  ungroup()

unique(diff$days)

diff_red <- diff %>% dplyr::select(event_id, asset_id, days)

df <- df %>%
  left_join(diff_red, by="event_id") %>%
  dplyr::select(-asset_id.y) %>%
  rename(asset_id = asset_id.x)

# #overview numbers
#     t <- df %>%
#       group_by(material) %>%
#       summarise(total=sum(sum)) %>%
#       ungroup() %>%
#       mutate(pct = total/87406 * 100) #plastics ~70% of macro debris
# 
#     t <- df %>%
#       group_by(item) %>%
#       summarise(total=sum(sum)) %>%
#       mutate(pct = total/87406 * 100) #cigarettes >25% of macro debris
    
```
# Percentage of items by land use
We'd discussed recreating this with errorbars, however I'm not sure how that would work as these values represent the sum total of items in each of these land uses?  We also discussed moving this to supplementary data, however I think it's actually a really important part of the story. While Covid impact may be a hot topic currently, I don't want the land use results to get lost in the Covid hype, especially because I haven't found any other papers about land use and stormwater debris. Also, the differences between land use are likely the most relevant part of this data in terms of implications for debris management efforts. 
```{r heatmap-item-top10, warning=FALSE, message=FALSE}

top10 <- df %>%
  filter(cycle %in% c("C1", "C2", "C3", "C4")) %>%
  filter(item != "Metal scrap") %>% #removing outlier entry
  group_by(item) %>%
  summarise(total=sum(sum)) %>%
  arrange(desc(total)) %>%
  top_n(10) %>%
  dplyr::select(item)

top10 <- as.vector(unlist(top10))

top_LUZ <- df %>%
  filter(cycle %in% c("C1", "C2", "C3", "C4")) %>%
  group_by(LUZ) %>%
  mutate(total_LUZ=sum(sum)) %>%
  ungroup() %>%
  group_by(LUZ, item) %>%
  mutate(total_itemLUZ = sum(sum)) %>%
  ungroup() %>%
  mutate(pct = (total_itemLUZ/total_LUZ)*100) %>%
  filter(item %in% top10) %>%
  dplyr::select(item, LUZ, pct) %>%
  unique() %>%
  spread(item, pct) %>%
  mutate(LUZ = as.factor(fct_relevel(LUZ, "CBD", "Shopping Centre", "Public Transport Terminal", "Industrial Precinct"))) %>%
  arrange(LUZ) %>%
  column_to_rownames(var="LUZ")
  
mat <- data.matrix(top_LUZ) #matrix for heatmap
# mat <- reorder_mat(mat = mat, order = order)
mat <- round(mat,0)
mat2 = ifelse(mat <= 0.99,"<1",as.character(round(mat,2))) #matrix for labels


col_ha <- HeatmapAnnotation(Region = anno_text(colnames(mat2), location = 1, rot = 60, 
                              just = "right"))

heatmap <- Heatmap(log10(mat+0.1), 
        name = "Debris count (percentage per land use zone)", 
        heatmap_legend_param  = list(color_bar = "continuous", at = c(-2.6,3.5),
                      title = "Debris counts (% total)",labels = c("Low", "High"),
                      grid_width = unit(0.8, "cm"),
                      legend_height = unit(5, "cm"),
                      title_position = "leftcenter-rot"), 
        col= colorRampPalette(brewer.pal(8, "PRGn"))(25),
        row_dend_reorder = TRUE,
        row_order = rownames(mat2),
        show_column_dend = FALSE,
        show_row_dend = FALSE,
        bottom_annotation = col_ha,
        show_row_names = TRUE,
        row_names_side = "left",
        show_column_names = FALSE,
        border = TRUE,
        column_title = "Percentage of top 10 debris items per land use zone",
        #percentage label
        cell_fun = function(j, i, x, y, width, height, fill) {
        grid.text(sprintf("%s", mat2[i, j],"%"), x, y, gp = gpar(fontsize = 10))},
        )

svg(filename="images/heatmap.svg", width = 12, height = 7)
draw(heatmap)
dev.off()

```

```{r heatmapcovid-item-top10, warning=FALSE, message=FALSE}

top10 <- df %>%
  filter(item != "Metal scrap") %>% #removing outlier entry
  filter(cycle %in% c("C5", "C6", "C7", "C9")) %>%
  group_by(item) %>%
  summarise(total=sum(sum)) %>%
  arrange(desc(total)) %>%
  top_n(10) %>%
  dplyr::select(item)

top10 <- as.vector(unlist(top10))

top_LUZ <- df %>%
  filter(cycle %in% c("C5", "C6", "C7", "C9")) %>%
  group_by(LUZ) %>%
  mutate(total_LUZ=sum(sum)) %>%
  ungroup() %>%
  group_by(LUZ, item) %>%
  mutate(total_itemLUZ = sum(sum)) %>%
  ungroup() %>%
  mutate(pct = (total_itemLUZ/total_LUZ)*100) %>%
  filter(item %in% top10) %>%
  dplyr::select(item, LUZ, pct) %>%
  unique() %>%
  spread(item, pct) %>%
  mutate(LUZ = as.factor(fct_relevel(LUZ, "CBD", "Shopping Centre", "Public Transport Terminal", "Industrial Precinct"))) %>%
  arrange(LUZ) %>%
  column_to_rownames(var="LUZ")
  
mat <- data.matrix(top_LUZ) #matrix for heatmap
# mat <- reorder_mat(mat = mat, order = order)
mat <- round(mat,0)
mat2 = ifelse(mat <= 0.99,"<1",as.character(round(mat,2))) #matrix for labels


col_ha <- HeatmapAnnotation(Region = anno_text(colnames(mat2), location = 1, rot = 60, 
                              just = "right"))

heatmapcovid <- Heatmap(log10(mat+0.1), 
        name = "Debris count (percentage per land use zone)", 
        heatmap_legend_param  = list(color_bar = "continuous", at = c(-2.6,3.5),
                      title = "Debris counts (% total)",labels = c("Low", "High"),
                      grid_width = unit(0.8, "cm"),
                      legend_height = unit(5, "cm"),
                      title_position = "leftcenter-rot"), 
        col= colorRampPalette(brewer.pal(8, "PRGn"))(25),
        row_dend_reorder = TRUE,
        row_order = rownames(mat2),
        show_column_dend = FALSE,
        show_row_dend = FALSE,
        bottom_annotation = col_ha,
        show_row_names = TRUE,
        row_names_side = "left",
        show_column_names = FALSE,
        border = TRUE,
        column_title = "Percentage of top 10 debris items per land use zone",
        #percentage label
        cell_fun = function(j, i, x, y, width, height, fill) {
        grid.text(sprintf("%s", mat2[i, j],"%"), x, y, gp = gpar(fontsize = 10))},
        )

svg(filename="images/heatmapcovid.svg", width = 12, height = 7)
draw(heatmapcovid)
dev.off()

```


## Timeline of debris item count


Top 15 items + OH&S through Covid-19 lockdowns.
```{r avg-items-per-lockdown, warning=FALSE, message=FALSE}
#find top 15 items
top15 <- df %>%
  filter(item != "Metal scrap") %>% #removing outlier entry
  group_by(item) %>%
  summarise(sum=sum(sum)) %>%
  arrange(desc(sum)) %>%
  top_n(15, sum)
  
#create vector from top 15 items
top15 <- top15$item 
top15 <- unlist(top15)

pol_items <- df %>%
  filter(item == "OH & S" | item=="Aluminium cans" | item=="Beer bottles (and similar)" | item == "Foam take-away")

#create vector from policy items
pol_items <- pol_items$item 
pol_items <- unlist(pol_items)

item_per_cycle <- df %>%
  filter(item %in% top15 | item %in% pol_items) %>%
  group_by(date, covid, LUZ, item) %>%
  summarise(cycle_sum = sum(sum)) %>%
  mutate(LGAcycle=ifelse(date=="2020-10-12", 2, 6), #create column of # of participating LGAs
         LGAsum=cycle_sum/LGAcycle, #divide by number of participating LGAs
         LGA_helper=ifelse(LGAcycle == 2, 2, 1), #take into consideration days between survey
         LGA_final_sum = LGAsum/LGA_helper #last survey round twice as long, so divide in half
         )

item_order <- item_per_cycle %>%
  group_by(item) %>%
  mutate(total=sum(LGAsum)) %>%
  arrange(desc(total)) %>%
  dplyr::select(item, total) %>%
  distinct

#create vector from order items
order <- item_order$item 
order <- unlist(order)

item_per_cycle$item <- factor(item_per_cycle$item, levels = order)
  
#set dates for covid levels
covid_lvl <- data.frame(name = c("1", "2"),
                   start = as.Date(c("2019-10-29", "2020-03-15")),
                   end = as.Date(c("2020-03-15", "2020-10-17")),
                   level = c("Pre-Covid", "During Covid"),
                   stringsAsFactors = FALSE) %>%
  mutate(median_x = start + floor((end-start)/2))


# p_timeseries <- ggplot() +
#   geom_rect(data=covid_lvl, aes(NULL, NULL, xmin=start, xmax=end, fill=level, 
#                                 ymin= -Inf, ymax=Inf), alpha=0.1) +
#   labs(fill="Covid-19 lockdown", linetype="Land Use Zones", colour="Land Use Zones",
#        x="Date", y="Debris count per LGA") +
#   # geom_point(data=item_per_cycle, aes(date, cycle_sum, color=LUZ)) +
#   geom_line(data=item_per_cycle, aes(date, LGA_final_sum, linetype=LUZ, color=LUZ)) +
#   theme(axis.text.x=element_text(angle=45, vjust=0.75, hjust=0.8)) +
#   facet_wrap(~item, scales="free_y") +
#   geom_vline(xintercept = as.numeric(as.Date("2020-06-30")), linetype=4)
# 
# p_timeseries

p_timeseries <- ggplot() +
  geom_rect(data=covid_lvl, aes(NULL, NULL, xmin=start, xmax=end, fill=level, 
                                ymin= -Inf, ymax=Inf), alpha=0.1) +
  labs(fill="Covid-19 lockdown", linetype="Land Use Zones", colour="Land Use Zones",
       x="Date", y="Debris count per LGA") +
  geom_line(data=item_per_cycle, aes(date, LGA_final_sum, linetype=LUZ, color=LUZ)) +
  theme(axis.text.x=element_text(angle=45, vjust=0.75, hjust=0.8)) +
  facet_wrap(~item, scales="free_y") +
  geom_vline(xintercept = as.numeric(as.Date("2020-06-30")), linetype=4) +
  theme(panel.grid = element_blank(),
        # legend.position = "none",
      axis.title = element_blank(),
      panel.background = element_blank())
  
  p_timeseries
  
svg("images/p_timeseries.svg", height=7, width=12)
plot(p_timeseries)
dev.off()

# p_timeseries_scale <- ggplot() +
#   geom_rect(data=covid_lvl, aes(NULL, NULL, xmin=start, xmax=end, fill=level, 
#                                 ymin= -Inf, ymax=Inf), alpha=0.1) +
#   labs(fill="Covid-19 lockdown", linetype="Land Use Zones", colour="Land Use Zones",
#        x="Date", y="Debris count per LGA") +
#   geom_line(data=item_per_cycle, aes(date, LGA_final_sum, linetype=LUZ, color=LUZ)) +
#   theme(axis.text.x=element_text(angle=45, vjust=0.75, hjust=0.8)) +
#   facet_wrap(~item)
#   # geom_vline(xintercept = as.numeric(as.Date("2020-06-30")), linetype=4)
#   # theme(panel.grid = element_blank(),
#   #       # legend.position = "none",
#   #     axis.title = element_blank(),
#   #     panel.background = element_blank())
#   
#   p_timeseries_scale
#   
# svg("images/p_timeseries_scale.svg", height=7, width=12)
# plot(p_timeseries_scale)
# dev.off()

#reference: https://plotly.com/ggplot2/geom_rect/ tutorial for geom_rect background

#Need to loop every LGA separately with the land uses for supplementary
```
##GLMM and predictions

I had a note to recreate this as a line graph through time, but that would necessitate changing the model and adding cycle or date as a fixed effect, which seems like I shouldn't do.  I tried several versions of that model and of a line graph, but none really made sense...
```{r data-mod, warning=FALSE, message=FALSE}
#aggregate data
mod <- df %>%
  group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
  summarise(sum=sum(sum)) %>%
  ungroup() %>%
  # filter(cycle %in% paste0("C",1:7)) %>%
  mutate(LGA = as.factor(LGA),
         LUZ = as.factor(fct_relevel(LUZ, "CBD", "Shopping Centre", "Public Transport Terminal", "Industrial Precinct")),
         covid = as.factor(covid),
         cycle = as.factor(cycle))
       
#create model
m  <- glmmTMB(sum ~ #Debris count per survey event
                LUZ * covid + #predicted by land use effected by covid
                (1|LGA/asset_id) + (1|cycle), #suburb random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod)

emmeans(m, ~covid|LUZ)
emmeans(m, pairwise~covid|LUZ, type="response")

#create predictor df
nd <- expand.grid(covid = unique(mod$covid),
                  LUZ = unique(mod$LUZ),
                  LGA = NA,
                  asset_id = NA,
                  cycle = NA)

#predict mean values of response variable
pred <- predict(object=m,
                newdata=nd,
                se.fit=T,
                re.form=NA)

#create standard error for graphing
nd$Total <- exp(pred$fit)
nd$SE_upper <- exp(pred$fit + pred$se.fit)
nd$SE_lower <- exp(pred$fit - pred$se.fit)

# New facet label names for covid variable
nd$covid <- factor(nd$covid, levels = c("1", "2"),
                  labels = c("Pre-Covid", "During Covid"))

# #create plot
# p_glmmtotal <- ggplot(nd, aes(y=Total, x=LUZ, fill=covid)) + 
#   geom_bar(position="dodge", stat="identity") +
#   geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2), position=position_dodge(width=0.90)) +
#   labs(title="Macro debris count (every 6 weeks)",
#        subtitle="By land use zone and covid restriction",
#        x="", y="Total debris items per asset", tag="") 
# 
# p_glmmtotal

#create plot
p_glmmtotal <- ggplot(nd, aes(y=Total, x=LUZ, fill=covid)) + 
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2), position=position_dodge(width=0.90)) +
  labs(title="Macro debris count (every 6 weeks)",
       subtitle="By land use zone and covid restriction",
       x="", y="Total debris items per asset", tag="")
    # theme(legend.position = "none",
    #     panel.grid = element_blank(),
    #     axis.title = element_blank(),
    #     axis.text = element_blank(),
    #     panel.background = element_blank())

p_glmmtotal

svg("images/p_glmmtotal.svg", height=7, width=12)
plot(p_glmmtotal)
dev.off()
```

# GLMM for top four items and OH&S


The above graph shows total debris count per asset every 6 weeks.  Because of lower numbers, here I've shown total debris count per LGA land use zone every 6 weeks. 
```{r data-mod-top5, warning=FALSE, message=FALSE}
# Function loop: each item
#Top 4 plus OH&S
top5_txt<- df %>%
  filter(item %in% c("Cigarette butts", 
                     "Plastic packaging (food)",
                     "Plastic wrap (non-food)",
                     "Paper remnants",
                     "Soft plastic remnants",
                     "OH & S")) %>% 
  distinct(item)%>%
  pull(item) %>% 
  droplevels()
 
# Purrr
pred_df = purrr::map_df(top5_txt, function(x){
  
  temp_df = df %>% 
    filter(item == paste0(x)) %>%
    group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
    summarise(sum=sum(sum)) %>%
    ungroup() %>%
    # filter(cycle %in% paste0("C",1:7)) %>%
    mutate(LGA = as.factor(LGA),
           LUZ = as.factor(fct_relevel(LUZ, "CBD", "Shopping Centre", "Public Transport Terminal", "Industrial Precinct")),
           covid = as.factor(covid),
           cycle = droplevels(as.factor(cycle)),
           item = x)
 
 #create model
 m_temp  <- glmmTMB(sum ~ #Debris count per survey event
                   LUZ * covid + #predicted by land use effected by covid
                   (1|LGA/asset_id) + (1|cycle), #suburb random effect
                   family=nbinom2(), #negative binomial to deal with count data and zeros
                   data = temp_df)
 nd_temp <- expand.grid(covid = unique(temp_df$covid),
                       LUZ = unique(temp_df$LUZ),
                       LGA = NA,
                       asset_id = NA,
                       cycle = NA)
 
 #predict function uses the model and the new df to predict mean values of the response variable.  It predicts a value for each row in the new df.
 pred_temp <- predict(object=m_temp,
                     newdata=nd_temp,
                     se.fit=T,
                     re.form=NA,
                     type="response")
 
 #creating standard error for graphing
 nd_temp$Total <- pred_temp$fit
 nd_temp$SE_upper <- pred_temp$fit + pred_temp$se.fit
 nd_temp$SE_lower <- pred_temp$fit - pred_temp$se.fit
 nd_temp$item = x
 
 return(nd_temp)
 
 })

# New facet label names for covid variable
pred_df$covid <- factor(pred_df$covid, levels = c("1", "2"),
                  labels = c("Pre-Covid", "During Covid"))
pred_item_order <- pred_df %>%
  group_by(item) %>%
  mutate(total=sum(Total)) %>%
  arrange(desc(total)) %>%
  dplyr::select(item, total) %>%
  distinct
#create vector from order items
pred_order <- pred_item_order$item 
pred_order <- unlist(pred_order)
pred_df$item <- factor(pred_df$item, levels = pred_order)

#  #create plot
# p_all <- ggplot(pred_df, aes(y=Total, x=LUZ, fill=covid)) + 
#   geom_bar(position="dodge", stat="identity") +
#   geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2), position=position_dodge(width=0.90)) +
#   labs(title="Debris count (every 6 weeks)",
#        subtitle="By land use and covid restriction",
#        x="", y="Total debris items per asset", tag="") +
#   facet_wrap(~item, scales="free_y") +
#   guides(x =  guide_axis(angle = 45)) 
# p_all

 #create plot
p_all <- ggplot(pred_df, aes(y=Total, x=LUZ, fill=covid)) + 
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2), position=position_dodge(width=0.90)) +
  labs(title="Debris count (every 6 weeks)",
       subtitle="By land use and covid restriction",
       x="", y="Total debris items per asset", tag="") +
  facet_wrap(~item) +
  guides(x =  guide_axis(angle = 45)) +
      theme(legend.position = "none",
        panel.grid = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank(),
        panel.background = element_blank())
p_all

svg("images/p_all.svg", height=7, width=12)
plot(p_all)
dev.off()
 
```


```{r data-mod-all-top, warning=FALSE, message=FALSE}
# Function loop: each item
#All top + items of interest
top_txt<- df %>%
  filter(item %in% top15 | item %in% pol_items) %>%
  distinct(item)%>%
  pull(item) %>% 
  droplevels()
 
# Purrr
pred_df = purrr::map_df(top_txt, function(x){
  
  temp_df = df %>% 
    filter(item == paste0(x)) %>%
    group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
    summarise(sum=sum(sum)) %>%
    ungroup() %>%
    # filter(cycle %in% paste0("C",1:7)) %>%
    mutate(LGA = as.factor(LGA),
           LUZ = as.factor(fct_relevel(LUZ, "CBD", "Shopping Centre", "Public Transport Terminal", "Industrial Precinct")),
           covid = as.factor(covid),
           cycle = droplevels(as.factor(cycle)),
           item = x)
 
 #create model
 m_temp  <- glmmTMB(sum ~ #Debris count per survey event
                   LUZ * covid + #predicted by land use effected by covid
                   (1|LGA/asset_id) + (1|cycle), #suburb random effect
                   family=nbinom2(), #negative binomial to deal with count data and zeros
                   data = temp_df)
 nd_temp <- expand.grid(covid = unique(temp_df$covid),
                       LUZ = unique(temp_df$LUZ),
                       LGA = NA,
                       asset_id = NA,
                       cycle = NA)
 
 #predict function uses the model and the new df to predict mean values of the response variable.  It predicts a value for each row in the new df.
 pred_temp <- predict(object=m_temp,
                     newdata=nd_temp,
                     se.fit=T,
                     re.form=NA,
                     type="response")
 
 #creating standard error for graphing
 nd_temp$Total <- pred_temp$fit
 nd_temp$SE_upper <- pred_temp$fit + pred_temp$se.fit
 nd_temp$SE_lower <- pred_temp$fit - pred_temp$se.fit
 nd_temp$item = x
 
 return(nd_temp)
 
 })
# New facet label names for covid variable
pred_df$covid <- factor(pred_df$covid, levels = c("1", "2"),
                  labels = c("Pre-Covid", "During Covid"))
pred_item_order <- pred_df %>%
  group_by(item) %>%
  mutate(total=sum(Total)) %>%
  arrange(desc(total)) %>%
  dplyr::select(item, total) %>%
  distinct
#create vector from order items
pred_order <- pred_item_order$item 
pred_order <- unlist(pred_order)
pred_df$item <- factor(pred_df$item, levels = pred_order)
 #create plot
p_all <- ggplot(pred_df, aes(y=Total, x=LUZ, fill=covid)) + 
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2), position=position_dodge(width=0.90)) +
  labs(title="Debris count (every 6 weeks)",
       subtitle="By land use and covid restriction",
       x="", y="Total debris items per asset", tag="") +
  facet_wrap(~item, scales="free_y") +
  guides(x =  guide_axis(angle = 45)) 
p_all
 
```
# Percent change

```{r percent-change}

pct_change <- df %>%
  filter(item %in% top15 | item == "OH & S") %>%
  group_by(covid,item) %>%
  summarise(Total=sum(sum)) %>%
  spread(covid, Total) %>%
  rename(precovid=2, duringcovid=3) %>%
  mutate(diff=duringcovid-precovid,
         pct_change=diff/precovid*100) %>%
  mutate(pos = diff >= 0)

p_change <- pct_change %>%
  mutate(item=as.factor(item),
         item=fct_reorder(item, pct_change)) %>%
  ggplot( aes(x=pct_change, y=item, fill=pos)) +
  geom_bar(stat="identity")
  # theme(legend.position = "none",
  #       panel.grid = element_blank(),
  #       axis.title = element_blank(),
  #       axis.text = element_blank(),
  #       panel.background = element_blank()) 

p_change

svg("images/p_change.svg", height=7, width=12)
plot(p_change)
dev.off()

  
```


# MDS plot


I've found that the MDS plot changes a lot depending on what variables I use to aggregate.  Here I've used all the variables from the model except asset_ID, as the unique assets created a lot of background noise.
```{r MDS-LGA-top15, warning=FALSE, message=FALSE, include=FALSE, results='hide'}
#find top 15 items
top15 <- df %>%
  filter(item!="Metal scrap") %>%
  group_by(item) %>%
  summarise(sum=sum(sum)) %>%
  arrange(desc(sum)) %>%
  top_n(15, sum)
  
#create vector from top 15 items
top15 <- top15$item 
top15 <- unlist(top15)


df_top15 <- df %>%
  filter(item %in% top15)

#create abundance long format
 abund_long <- df_top15 %>%
    group_by(LGA, LUZ, covid, item) %>% 
    summarise(sum=sum(sum)) %>% #total of each item type per survey event
  ungroup()

#convert to data wide
  abund <- spread(abund_long, item, sum) 

  
  com <- abund[4:18]
  
  #removing debris items that are zero across all surveys
  com <- com[, which(colSums(com) != 0)]
  
  ord <- metaMDS(com)
  
  ord$stress
  
  data.scores <- as.data.frame(scores(ord)) #create df of ord points
  data.scores$LUZ <- abund$LUZ
  data.scores$covid <- abund$covid
  data.scores <- data.scores %>%
    mutate(covid = as.factor(covid),
           LUZ = as.factor(LUZ))
  
  species.scores <- as.data.frame(scores(ord, "species"))
  species.scores$item <- rownames(species.scores) #create item id column
  
  
# #Hull data LUZ
# CBD <- data.scores[data.scores$LUZ == "CBD", ][chull(data.scores[data.scores$LUZ ==
#     "CBD", c("NMDS1", "NMDS2")]), ]  # hull values for CBD
# Transport <- data.scores[data.scores$LUZ == "Public Transport Terminal", ][chull(data.scores[data.scores$LUZ ==
#     "Public Transport Terminal", c("NMDS1", "NMDS2")]), ]  # hull values for Transport
# Indus <- data.scores[data.scores$LUZ == "Industrial Precinct", ][chull(data.scores[data.scores$LUZ == "Industrial Precinct", c("NMDS1", "NMDS2")]), ]  # hull values for Industry
# Shop <- data.scores[data.scores$LUZ == "Shopping Centre", ][chull(data.scores[data.scores$LUZ ==
#     "Shopping Centre", c("NMDS1", "NMDS2")]), ]  # hull values for shopping
# 
# #create polygons
# LUZ.hull.data <- rbind(CBD, Transport, Indus, Shop)  #combine groups
# LUZ.hull.data
# 
# #Hull data covid
# covid1 <- data.scores[data.scores$covid == "1", ][chull(data.scores[data.scores$covid ==
#     "1", c("NMDS1", "NMDS2")]), ]  # hull values for Pre-Covid
# covid2 <- data.scores[data.scores$covid == "2", ][chull(data.scores[data.scores$covid ==
#     "2", c("NMDS1", "NMDS2")]), ]  # hull values for During Covid
# 
# #create polygons
# cov.hull.data <- rbind(covid1,covid2)  #combine groups
# cov.hull.data

# New facet label names for covid variable
data.scores$covid <- factor(data.scores$covid, levels = c("1", "2"),
                  labels = c("Pre-Covid", "During Covid"))

#creating ellipses

#add group column to data.scores
NMDS <- data.frame(NMDS1 = ord$points[,1], NMDS2 = ord$points[,2],group=data.scores$LUZ)

 veganCovEllipse<-function (cov, center = c(0, 0), scale = 1, npoints = 100) 
  {
    theta <- (0:npoints) * 2 * pi/npoints
    Circle <- cbind(cos(theta), sin(theta))
    t(center + scale * t(Circle %*% chol(cov)))
  }

  df_ell <- data.frame()
  for(g in levels(NMDS$group)){
    df_ell <- rbind(df_ell, cbind(as.data.frame(with(NMDS[NMDS$group==g,],
                    veganCovEllipse(cov.wt(cbind(NMDS1,NMDS2),wt=rep(1/length(NMDS1),length(NMDS1)))$cov,center=c(mean(NMDS1),mean(NMDS2)))))
                    ,group=g))
  }
# #ellipses for covid  
#   #add group column to data.scores
# cov_NMDS <- data.frame(NMDS1 = ord$points[,1], NMDS2 = ord$points[,2],group=data.scores$covid)
# 
#  veganCovEllipse<-function (cov, center = c(0, 0), scale = 1, npoints = 100) 
#   {
#     theta <- (0:npoints) * 2 * pi/npoints
#     Circle <- cbind(cos(theta), sin(theta))
#     t(center + scale * t(Circle %*% chol(cov)))
#   }
# 
#   df_ell_cov <- data.frame()
#   for(g in levels(cov_NMDS$group)){
#     df_ell_cov <- rbind(df_ell_cov, cbind(as.data.frame(with(cov_NMDS[cov_NMDS$group==g,],
#                     veganCovEllipse(cov.wt(cbind(NMDS1,NMDS2),wt=rep(1/length(NMDS1),length(NMDS1)))$cov,center=c(mean(NMDS1),mean(NMDS2)))))
#                     ,group=g))
#   }
  
  #The first method is calculating the ellipse paths using standard deviation and no scaling. The second method is using standard error and is also scaling the data. Thus, the plot produced with the first method can also be achieved with the second method by making the necessary changes to the ordiellipse function (kind='sd', not 'se'), and removing the scale (ord[[g]]$scale) from the veganCovEllipse function.
  #https://stackoverflow.com/questions/42799838/follow-up-plotting-ordiellipse-function-from-vegan-package-onto-nmds-plot-creat
```

```{r MDS-plot, warning=FALSE, message=FALSE}
#MDS plot LUZ
p_mds <- ggplot() +
  geom_point(data=data.scores, 
             aes(x=NMDS1, y=NMDS2, colour=LUZ, shape=covid), alpha=0.8) + 
  geom_path(data=df_ell, aes(x=NMDS1, y=NMDS2,colour=group),
            size=1, linetype=2) +
  coord_equal() +
  theme_bw()

p_mds

svg("images/p_mds.svg", height=7, width=7)
plot(p_mds)
dev.off()

p_mds_txt <- ggplot() +
  geom_text(data=species.scores,aes(x=NMDS1,y=NMDS2, label=item)) +
  geom_point(data=data.scores, 
             aes(x=NMDS1, y=NMDS2, colour=LUZ, shape=covid), alpha=0.8) + 
  geom_path(data=df_ell, aes(x=NMDS1, y=NMDS2,colour=group), 
            size=1, linetype=2) +
  coord_equal() +
  theme_bw()

svg("images/p_mds_txt.svg", height=7, width=7)
plot(p_mds_txt)
dev.off()

p_mds
p_mds_txt

## MDS plot covid
# ggplot() +
#   geom_text(data=species.scores,aes(x=NMDS1,y=NMDS2, label=item)) +
#   geom_point(data=data.scores,
#              aes(x=NMDS1, y=NMDS2, colour=covid, shape=LUZ), alpha=0.8) +
#     geom_path(data=df_ell_cov, aes(x=NMDS1, y=NMDS2,colour=group), 
#             size=1, linetype=2) +
#   coord_equal() +
#   theme_bw()
```

# Testing community data


```{r adonis-pairwise, warning=FALSE, message=FALSE}

#Both land use and covid have significant scores, but there is not a significant interaction between them
df.adonis <- adonis(com ~ data.scores$LUZ*data.scores$covid, permutations=999, method="bray")
df.adonis

#testing land use dispersion
df.dis <- vegdist(com, method = "bray") # make a distance matrix
df.betadisper.luz <- betadisper(df.dis, data.scores$LUZ) # test dispersion
df.betadisper.luz
permutest(df.betadisper.luz) #dispersion not significant

#testing covid dispersion
df.dis <- vegdist(com, method = "bray") # make a distance matrix
df.betadisper.cov <- betadisper(df.dis, data.scores$covid) # test dispersion
df.betadisper.cov
permutest(df.betadisper.cov) #dispersion not significant

citation("pairwiseAdonis")
```