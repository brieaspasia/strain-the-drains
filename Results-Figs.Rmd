---
title: "Results-Figs"
author: "Brie Sherow"
date: "05/04/2021"
output: 
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    df_print: paged
  pdf_document:
    toc: yes
---

```{r setup, warning=FALSE, message=FALSE, results='hide', include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, warning=FALSE, message=FALSE, results='hide', include=FALSE}
library(ggplot2) #graphing
library(ggthemes) #graphing templates
library(hrbrthemes) #graphing templates
library(lubridate) #date manipulation
library(forcats) #working with factors
library(tidyverse) #manipulating data
library(knitr) #rmarkdown functions
library(tidyr) #long & wide formats
library(reshape2) #melt function for wide and long formats
library(stats) #R stats functions
library(broom) #create summaries from stats objects
library(car) #lm regression
library(MASS) #stats
library(lme4) #glmer function
library(DHARMa) #testing model diagnostics
library(glmmTMB) #fit zero-inflated negative binomial
# library(lattice) #fourth corner heatmap
# library(corrplot) #co-occurrence matrix
library(gclus) #co-occurrence matrix
library(broom.mixed) #regression tables
library(vegan) #ordination
library(emmeans)
library(ComplexHeatmap)
library(RColorBrewer)
library(cowplot) #minimal backgrounds for ggplot
library(purrr) #looping
```

```{r create-df, warning=FALSE, message=FALSE, results='hide', include=FALSE}
#create survey count  

        #load item counts
        item <- read.csv(file="Data/2101_item.csv", 
                         header=T, sep=",") 

#remove duplicate entries
        item <- unique(item[!duplicated(item),])

        #load item labels
        item_label <- read.csv(file="Data/item_label.csv", 
                         header=T, sep=",") 
        
        #load events
        event <- read.csv(file="Data/2101_event.csv", 
                         header=T, sep=",") 
        
        #join event info to item count
        survey_count <- left_join(item, event, by="event_id")
        
        survey_count <- survey_count %>%
          dplyr::select(-event.tot, -event.wt, -vol, -hr, -event_date, -note) %>%
          filter(item!="Pollution Rating") 

        #summarise duplicate survey entries (this will lose one set of notes....)
        item_merge <- survey_count %>%
          group_by(event_id, item) %>%
          summarise(sum=sum(total)) %>%
         ungroup()

        #join survey count to the merged summaries
        survey_count <- item_merge %>%
          left_join(survey_count, by=c("event_id"="event_id", "item"="item")) %>%
          dplyr::select(-total) %>%
          unique()
        
#create abundance data with all possible debris items and zero values

      # #count of items per unique event
      # event_count <- survey_count %>%
      #   group_by(event_id, item) %>%
      #   summarise(sum=sum(sum)) %>%
      #   ungroup()

      #load AMDI code\
      AMDI_code <- read.csv(file="Data/200101_AMDI_code.csv", 
                       header=T, sep=",") 
      
      #create full list of possible items
      AMDI <- AMDI_code %>%
        filter(item_code_id != "LSTD1") %>% #remove pollution rating / microplastics
        dplyr::select(item.ex, mat) #column of all possible item types
      
      
      #create a df of all possible items at all survey events, complete with zeros
      abund <- full_join(AMDI, survey_count, by=c("item.ex" = "item"))
      
      #data long to wide
      abund_wide <- abund %>%
        spread(item.ex, sum) %>%
        dplyr::select(-Asset.ID, -Cycle, -covid, -date, -mat)
      
      #replace na values with 0
      abund_wide[is.na(abund_wide)] <- 0
        
      #transform back to long but now with the full items list and zero data
      abund_long <- melt(abund_wide, id.vars="event_id")
      
      #remove the zero event ID (how did that get there?)
      abund_long <- abund_long[abund_long$event_id != 0, ]

#join abundance data to survey notes and other columns
      df <- left_join(abund_long, event, by="event_id")
      
        #load sites
          site <- read.csv(file="Data/200619_site.csv", 
                           header=T, sep=",") 
      
            #join LGA and luz to df
          df <- left_join(df, site, by = "Asset.ID")
          
          #classify items correctly      
           df <- df %>%
        dplyr::select(-event_date, -event.tot, -event.wt, -vol, -hr, 
                      -Site, -Latitude, -Longitude) %>% #remove duplicate columns
        rename(item=variable, #rename columns
               sum=value,
               asset_id = Asset.ID,
               cycle=Cycle,
               LUZ = Land.use.zone) %>% 
        mutate(asset_id=as.factor(asset_id), #classify columns correctly  
             item=as.factor(item),
             LGA=as.factor(LGA),
             LUZ=as.factor(LUZ),
             event_id=as.factor(event_id),
             cycle=as.factor(cycle),
             date=dmy(date)
             )

#resolve duplicates in MC01, C5    
          dup <- df %>% 
            group_by(asset_id, cycle, item) %>% #reconcile duplicates in MC01 Cycle 5
            summarise(sum=sum(sum)) %>%
            ungroup()
           
      # #check difference
      #      subsetdf <- df %>%
      #        dplyr::select(Asset.ID, Cycle, item, sum)
      #      
      #      setdiff(subsetdf, dup)
           
      #join C5 correction to df
      df <- dup %>% left_join(df, by=c("asset_id"="asset_id", 
                                       "cycle"="cycle", "item"="item")) %>%
        dplyr::select(-sum.y) %>%
        filter(event_id != "23097") %>%
        rename(sum = sum.x) %>%
        distinct()
      
      item_labelDF <- df %>%
        distinct(item)
      
      df <- df %>%
        left_join(item_label, by=c("item" = "item.type")) %>%
        mutate(item = A3) %>%
        dplyr::select(-A3)
      
#classify columns correctly    
      df <- df %>%
      mutate(item=as.factor(item),
             covid=as.factor(covid),
             material=as.factor(material))

#Combine C8 and C9    
     #isolate C8
    C8 <- df %>%
      filter(cycle == "C8")
    
    #isolate C9
    C9 <- df %>%
      filter(cycle == "C9")
    
    #join C8 and C9 by unique drain and item to capture double surveys
    t <- left_join(C9, C8, by=c("asset_id"="asset_id", "item" = "item"))
    t$sum.y[is.na(t$sum.y)] <- 0 #change na to 0
    
    t <- t %>%
      mutate(combined_sum = sum.y + sum.x) %>% #add C8 and C9 together
      dplyr::select(event_id.x, item, combined_sum)
    
    df <- df %>% 
      left_join(t, by = c("event_id"="event_id.x", "item"="item")) %>%
      filter(cycle != "C8") %>%
      mutate(combined_sum = replace_na(combined_sum, 0),
              sum2 = ifelse(cycle != "C9", sum, 0),
              final_sum = sum2 + combined_sum) %>%
      dplyr::select(-sum2, -sum, -combined_sum) %>%
      rename(sum = final_sum)
    
#find days since last survey at each asset
diff <- df %>%
  distinct(event_id, .keep_all = T) %>%
  dplyr::select(-item,-sum) %>%
  group_by(asset_id) %>% #looking at each event
  arrange(date) %>%
  mutate(days = date - lag(date)) %>% #days since last survey event
  filter(days != "NA") %>% #remove first survey date
  ungroup()

unique(diff$days)

diff_red <- diff %>% dplyr::select(event_id, asset_id, days)

df <- df %>%
  left_join(diff_red, by="event_id") %>%
  dplyr::select(-asset_id.y) %>%
  rename(asset_id = asset_id.x)

# #overview numbers
#     t <- df %>%
#       group_by(material) %>%
#       summarise(total=sum(sum)) %>%
#       ungroup() %>%
#       mutate(pct = total/87406 * 100) #plastics ~70% of macro debris
# 
#     t <- df %>%
#       group_by(item) %>%
#       summarise(total=sum(sum)) %>%
#       mutate(pct = total/87406 * 100) #cigarettes >25% of macro debris
    
```

## Timeline of debris item count


Top 15 items + OH&S through Covid-19 lockdowns.
```{r avg-items-per-lockdown, warning=FALSE, message=FALSE}
#find top 15 items
top15 <- df %>%
  filter(item != "Metal scrap") %>% #removing outlier entry
  group_by(item) %>%
  summarise(sum=sum(sum)) %>%
  arrange(desc(sum)) %>%
  top_n(15, sum)
  
#create vector from top 15 items
top15 <- top15$item 
top15 <- unlist(top15)

pol_items <- df %>%
  filter(item == "OH & S" | item=="Aluminium cans" | item=="Beer bottles (and similar)" | item == "Foam take-away")

#create vector from policy items
pol_items <- pol_items$item 
pol_items <- unlist(pol_items)

item_per_cycle <- df %>%
  filter(item %in% top15 | item %in% pol_items) %>%
  group_by(date, covid, LUZ, item) %>%
  summarise(cycle_sum = sum(sum)) %>%
  mutate(LGAcycle=ifelse(date=="2020-10-12", 2, 6), #create column of # of participating LGAs
         LGAsum=cycle_sum/LGAcycle, #divide by number of participating LGAs
         LGA_helper=ifelse(LGAcycle == 2, 2, 1), #take into consideration days between survey
         LGA_final_sum = LGAsum/LGA_helper #last survey round twice as long, so divide in half
         )

item_order <- item_per_cycle %>%
  group_by(item) %>%
  mutate(total=sum(LGAsum)) %>%
  arrange(desc(total)) %>%
  dplyr::select(item, total) %>%
  distinct

#create vector from order items
order <- item_order$item 
order <- unlist(order)

item_per_cycle$item <- factor(item_per_cycle$item, levels = order)
  
#set dates for covid levels
covid_lvl <- data.frame(name = c("1", "2"),
                   start = as.Date(c("2019-10-29", "2020-03-15")),
                   end = as.Date(c("2020-03-15", "2020-10-17")),
                   level = c("Pre-Covid", "During Covid"),
                   stringsAsFactors = FALSE) %>%
  mutate(median_x = start + floor((end-start)/2))


# p_timeseries <- ggplot() +
#   geom_rect(data=covid_lvl, aes(NULL, NULL, xmin=start, xmax=end, fill=level, 
#                                 ymin= -Inf, ymax=Inf), alpha=0.1) +
#   labs(fill="Covid-19 lockdown", linetype="Land Use Zones", colour="Land Use Zones",
#        x="Date", y="Debris count per LGA") +
#   # geom_point(data=item_per_cycle, aes(date, cycle_sum, color=LUZ)) +
#   geom_line(data=item_per_cycle, aes(date, LGA_final_sum, linetype=LUZ, color=LUZ)) +
#   theme(axis.text.x=element_text(angle=45, vjust=0.75, hjust=0.8)) +
#   facet_wrap(~item, scales="free_y") +
#   geom_vline(xintercept = as.numeric(as.Date("2020-06-30")), linetype=4)
# 
# p_timeseries

p_timeseries <- ggplot() +
  geom_rect(data=covid_lvl, aes(NULL, NULL, xmin=start, xmax=end, fill=level, 
                                ymin= -Inf, ymax=Inf), alpha=0.1) +
  labs(fill="Covid-19 lockdown", linetype="Land Use Zones", colour="Land Use Zones",
       x="Date", y="Debris count per LGA") +
  geom_line(data=item_per_cycle, aes(date, LGA_final_sum, linetype=LUZ, color=LUZ)) +
  theme(axis.text.x=element_text(angle=45, vjust=0.75, hjust=0.8)) +
  facet_wrap(~item, scales="free_y") +
  # geom_vline(xintercept = as.numeric(as.Date("2020-06-30")), linetype=4) +
  theme(panel.grid = element_blank(),
        # legend.position = "none",
      axis.title = element_blank(),
      panel.background = element_blank())
  
  p_timeseries
  
# svg("images/p_timeseries.svg", height=7, width=12)
# plot(p_timeseries)
# dev.off()

#reference: https://plotly.com/ggplot2/geom_rect/ tutorial for geom_rect background

#Need to loop every LGA separately with the land uses for supplementary
```
##GLMM and predictions

```{r data-mod, warning=FALSE, message=FALSE}
#aggregate data
mod <- df %>%
  group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
  summarise(sum=sum(sum)) %>%
  ungroup() %>%
  # filter(cycle %in% paste0("C",1:7)) %>%
  mutate(LGA = as.factor(LGA),
         LUZ = as.factor(fct_relevel(LUZ, "CBD", "Shopping Centre", "Public Transport Terminal", "Industrial Precinct")),
         covid = as.factor(covid),
         cycle = as.factor(cycle))
       
#create model
m  <- glmmTMB(sum ~ #Debris count per survey event
                LUZ * covid + #predicted by land use effected by covid
                (1|LGA/asset_id) + (1|cycle), #suburb random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod)

pairs.cl.emm <- emmeans(m, pairwise~covid|LUZ, type="response", level=0.95, infer=T)
pairs.lc.emm <- emmeans(m, pairwise~LUZ|covid, type="response", level=0.95, infer=T)
pairs.cl.emm
pairs.lc.emm
test(pairs.emm, delta=log(1.05))

Anova(m)
joint_tests(m)


#create predictor df
nd <- expand.grid(covid = unique(mod$covid),
                  LUZ = unique(mod$LUZ),
                  LGA = NA,
                  asset_id = NA,
                  cycle = NA)

#predict mean values of response variable
pred <- predict(object=m,
                newdata=nd,
                se.fit=T,
                re.form=NA)

#create standard error for graphing
nd$Total <- exp(pred$fit)
nd$SE_upper <- exp(pred$fit + pred$se.fit)
nd$SE_lower <- exp(pred$fit - pred$se.fit)

# New facet label names for covid variable
nd$covid <- factor(nd$covid, levels = c("1", "2"),
                  labels = c("Pre-Covid", "During Covid"))

# #create plot
# p_glmmtotal <- ggplot(nd, aes(y=Total, x=LUZ, fill=covid)) + 
#   geom_bar(position="dodge", stat="identity") +
#   geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2), position=position_dodge(width=0.90)) +
#   labs(title="Macro debris count (every 6 weeks)",
#        subtitle="By land use zone and covid restriction",
#        x="", y="Total debris items per asset", tag="") 
# 
# p_glmmtotal

#create plot
p_glmmtotal <- ggplot(nd, aes(y=Total, x=LUZ, fill=covid)) + 
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2), position=position_dodge(width=0.90)) +
  labs(title="Macro debris count (every 6 weeks)",
       subtitle="By land use zone and covid restriction",
       x="", y="Total debris items per asset", tag="")
    # theme(legend.position = "none",
    #     panel.grid = element_blank(),
    #     axis.title = element_blank(),
    #     axis.text = element_blank(),
    #     panel.background = element_blank())

p_glmmtotal

svg("images/p_glmmtotal.svg", height=7, width=12)
plot(p_glmmtotal)
dev.off()
```

# GLMM for top four items and OH&S

The above graph shows total debris count per asset every 6 weeks.  Because of lower numbers, here I've shown total debris count per LGA land use zone every 6 weeks. 
```{r data-mod-top5, warning=FALSE, message=FALSE}
# Function loop: each item
#Top 4 plus OH&S
top5_txt<- df %>%
  filter(item %in% c("Cigarette butts", 
                     "Plastic packaging (food)",
                     "Plastic wrap (non-food)",
                     "Paper remnants",
                     "Soft plastic remnants",
                     "OH & S")) %>% 
  distinct(item)%>%
  pull(item) %>% 
  droplevels()
 
# Purrr
pred_df = purrr::map_df(top5_txt, function(x){
  
  temp_df = df %>% 
    filter(item == paste0(x)) %>%
    group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
    summarise(sum=sum(sum)) %>%
    ungroup() %>%
    # filter(cycle %in% paste0("C",1:7)) %>%
    mutate(LGA = as.factor(LGA),
           LUZ = as.factor(fct_relevel(LUZ, "CBD", "Shopping Centre", "Public Transport Terminal", "Industrial Precinct")),
           covid = as.factor(covid),
           cycle = droplevels(as.factor(cycle)),
           item = x)
 
 #create model
 m_temp  <- glmmTMB(sum ~ #Debris count per survey event
                   LUZ * covid + #predicted by land use effected by covid
                   (1|LGA/asset_id) + (1|cycle), #suburb random effect
                   family=nbinom2(), #negative binomial to deal with count data and zeros
                   data = temp_df)
 nd_temp <- expand.grid(covid = unique(temp_df$covid),
                       LUZ = unique(temp_df$LUZ),
                       LGA = NA,
                       asset_id = NA,
                       cycle = NA)
 
 #predict function uses the model and the new df to predict mean values of the response variable.  It predicts a value for each row in the new df.
 pred_temp <- predict(object=m_temp,
                     newdata=nd_temp,
                     se.fit=T,
                     re.form=NA,
                     type="response")
 
 #creating standard error for graphing
 nd_temp$Total <- pred_temp$fit
 nd_temp$SE_upper <- pred_temp$fit + pred_temp$se.fit
 nd_temp$SE_lower <- pred_temp$fit - pred_temp$se.fit
 nd_temp$item = x
 
 return(nd_temp)
 
 })

# New facet label names for covid variable
pred_df$covid <- factor(pred_df$covid, levels = c("1", "2"),
                  labels = c("Pre-Covid", "During Covid"))
pred_item_order <- pred_df %>%
  group_by(item) %>%
  mutate(total=sum(Total)) %>%
  arrange(desc(total)) %>%
  dplyr::select(item, total) %>%
  distinct
#create vector from order items
pred_order <- pred_item_order$item 
pred_order <- unlist(pred_order)
pred_df$item <- factor(pred_df$item, levels = pred_order)

#  #create plot
# p_all <- ggplot(pred_df, aes(y=Total, x=LUZ, fill=covid)) + 
#   geom_bar(position="dodge", stat="identity") +
#   geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2), position=position_dodge(width=0.90)) +
#   labs(title="Debris count (every 6 weeks)",
#        subtitle="By land use and covid restriction",
#        x="", y="Total debris items per asset", tag="") +
#   facet_wrap(~item, scales="free_y") +
#   guides(x =  guide_axis(angle = 45)) 
# p_all

 #create plot
p_all <- ggplot(pred_df, aes(y=Total, x=LUZ, fill=covid)) + 
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2), position=position_dodge(width=0.90)) +
  labs(title="Debris count (every 6 weeks)",
       subtitle="By land use and covid restriction",
       x="", y="Total debris items per asset", tag="") +
  facet_wrap(~item) +
  guides(x =  guide_axis(angle = 45)) +
      theme(legend.position = "none",
        panel.grid = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank(),
        panel.background = element_blank())
p_all

svg("images/p_all.svg", height=7, width=12)
plot(p_all)
dev.off()
 
```


```{r data-mod-all-top, warning=FALSE, message=FALSE}
# Function loop: each item
#All top + items of interest
top_txt<- df %>%
  filter(item %in% top15 | item %in% pol_items) %>%
  distinct(item)%>%
  pull(item) %>% 
  droplevels()
 
# Purrr
pred_df = purrr::map_df(top_txt, function(x){
  
  temp_df = df %>% 
    filter(item == paste0(x)) %>%
    group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
    summarise(sum=sum(sum)) %>%
    ungroup() %>%
    # filter(cycle %in% paste0("C",1:7)) %>%
    mutate(LGA = as.factor(LGA),
           LUZ = as.factor(fct_relevel(LUZ, "CBD", "Shopping Centre", "Public Transport Terminal", "Industrial Precinct")),
           covid = as.factor(covid),
           cycle = droplevels(as.factor(cycle)),
           item = x)
 
 #create model
 m_temp  <- glmmTMB(sum ~ #Debris count per survey event
                   LUZ * covid + #predicted by land use effected by covid
                   (1|LGA/asset_id) + (1|cycle), #suburb random effect
                   family=nbinom2(), #negative binomial to deal with count data and zeros
                   data = temp_df)
 
 nd_temp <- expand.grid(covid = unique(temp_df$covid),
                       LUZ = unique(temp_df$LUZ),
                       LGA = NA,
                       asset_id = NA,
                       cycle = NA)
 
 #predict function uses the model and the new df to predict mean values of the response variable.  It predicts a value for each row in the new df.
 pred_temp <- predict(object=m_temp,
                     newdata=nd_temp,
                     se.fit=T,
                     re.form=NA,
                     type="response")
 
 #creating standard error for graphing
 nd_temp$Total <- pred_temp$fit
 nd_temp$SE_upper <- pred_temp$fit + pred_temp$se.fit
 nd_temp$SE_lower <- pred_temp$fit - pred_temp$se.fit
 nd_temp$item = x
 
 return(nd_temp)
 
 })


# New facet label names for covid variable
pred_df$covid <- factor(pred_df$covid, levels = c("1", "2"),
                  labels = c("Pre-Covid", "During Covid"))
pred_item_order <- pred_df %>%
  group_by(item) %>%
  mutate(total=sum(Total)) %>%
  arrange(desc(total)) %>%
  dplyr::select(item, total) %>%
  distinct
#create vector from order items
pred_order <- pred_item_order$item 
pred_order <- unlist(pred_order)
pred_df$item <- factor(pred_df$item, levels = pred_order)
 #create plot
p_all <- ggplot(pred_df, aes(y=Total, x=LUZ, fill=covid)) + 
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2), position=position_dodge(width=0.90)) +
  labs(title="Debris count (every 6 weeks)",
       subtitle="By land use and covid restriction",
       x="", y="Total debris items per asset", tag="") +
  facet_wrap(~item, scales="free_y") +
  guides(x =  guide_axis(angle = 45)) 
p_all

pred_df10 <- pred_df %>%
  group_by(item) %>%
  summarise(Total=sum(Total)) %>%
  arrange(desc(Total)) %>%
  top_n(10, Total)

#create vector from top 15 items
pred_df10 <- pred_df10$item 
pred_df10 <- unlist(pred_df10)

pred_df_hm <- pred_df %>% 
  filter(item %in% pred_df10) %>%
  ungroup() %>%
  mutate(item=as.factor(item),
  item=fct_reorder(item, Total))
 
 p_pred_df_hm <- ggplot(pred_df_hm, aes(x=Total, y=item, fill=covid)) +
  geom_bar(position = position_dodge2(reverse=T), stat="identity") +
  geom_errorbar(aes(xmin=SE_lower, xmax=SE_upper, width = 0.9), 
                position=position_dodge2(reverse = T, width = 0.9)) +
  facet_wrap(~LUZ, scales = "free_x") +
    guides(x =  guide_axis(angle = 45))
      # theme(legend.position = "none",
      #   panel.grid = element_blank(),
      #   axis.title = element_blank(),
      #   axis.text = element_blank(),
      #   panel.background = element_blank())

p_pred_df_hm

svg("images/p_heatmap-replace.svg", height=7, width=12)
plot(p_pred_df_hm)
dev.off()

display.brewer.all()

 
```
```{r emmeans-loop}
# https://cran.r-project.org/web/packages/emmeans/vignettes/interactions.html

# Function loop: each item
#All top + items of interest
top_txt<- df %>%
  filter(item %in% top15 | item %in% pol_items) %>%
  distinct(item)%>%
  pull(item) %>% 
  droplevels()
 
# Purrr
emmeans_item = purrr::map_df(top_txt, function(x){
  
  temp_df = df %>% 
    filter(item == paste0(x)) %>%
    group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
    summarise(sum=sum(sum)) %>%
    ungroup() %>%
    mutate(LGA = as.factor(LGA),
           LUZ = as.factor(fct_relevel(LUZ, "CBD", "Shopping Centre", "Public Transport Terminal", "Industrial Precinct")),
           covid = as.factor(covid),
           cycle = droplevels(as.factor(cycle)),
           item = x)
 
 #create model
 m_temp  <- glmmTMB(sum ~ #Debris count per survey event
                   LUZ * covid + #predicted by land use effected by covid
                   (1|LGA/asset_id) + (1|cycle), #suburb random effect
                   family=nbinom2(), #negative binomial to deal with count data and zeros
                   data = temp_df)
 
j <- joint_tests(m_temp)
j$item = x

return(j)

 })

emmeans_item
# joint_tests() obtains and tests the interaction contrasts for all effects in the model and compiles them in one Type-III-ANOVA-like table
j<- joint_tests(m)
# model term df1 df2 F.ratio p.value
#  LUZ          3 867   5.771 0.0007 
#  covid        1 867   8.529 0.0036 
#  LUZ:covid    3 867  11.117 <.0001 
```


```{r emmeans-interactions-cigs}
mod_cigs <- df %>%
  filter(item=="Cigarette butts") %>%
  group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
  summarise(sum=sum(sum)) %>%
  ungroup() %>%
  mutate(LGA = as.factor(LGA),
         LUZ = as.factor(fct_relevel(LUZ, "CBD", "Shopping Centre", "Public Transport Terminal", "Industrial Precinct")),
         covid = as.factor(covid),
         cycle = as.factor(cycle))
       
#create model
m_temp  <- glmmTMB(sum ~ #Debris count per survey event
                LUZ * covid + #predicted by land use effected by covid
               (1|LGA/asset_id) + (1|cycle), #suburb random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod_cigs)

summary(m_temp)

pairs.cl.temp <- emmeans(m_temp, pairwise~covid|LUZ, type="response", level=0.95, infer=T)
pairs.lc.temp <- emmeans(m_temp, pairwise~LUZ|covid, type="response", level=0.95, infer=T)


as.data.frame(pairs.cl.temp)
pairs.lc.temp

cig.emm <- emmeans(m_cigs, pairwise~covid|LUZ, level=0.95, infer=T)
summary(cig.emm)



```

# Percent change

```{r percent-change-delta-luz}
#refer to delta method
#https://stats.stackexchange.com/questions/64652/calculating-standard-deviation-associated-with-percentage-change
pct_change <- df %>%
  filter(item %in% top15 | item == "OH & S") %>% #select items
  group_by(covid, LUZ, item) %>% #look at items per asset both cov
  mutate(luzMean=mean(sum)) %>% #mean of item btwn 8 cycles at each asset per covid level
  ungroup() %>%
  dplyr::select(covid, LUZ, item, luzMean) %>% #select vars
  unique() #remove duplicates from survey cycle
  
# Split data into each item
pct_change <- droplevels(pct_change)
ls.data_split <- split(pct_change, f = pct_change$item)


luz_delta = purrr::map_df(ls.data_split, function(x){

x1 = x %>% filter(covid == 1)

x2 = x %>% filter(covid == 2) %>% dplyr::select(LUZ, luzMean)

xV = var(x1$luzMean)
x_mean = mean(x1$luzMean)

yV = var(x2$luzMean)
y_mean = mean(x2$luzMean)

num = (yV*x_mean^2) - 2*cov(x1$luzMean, x2$luzMean)*x_mean*y_mean + (xV*y_mean^2)
denom = x_mean^4

se_pcc = 100*sqrt(num/denom)

pcc = ((y_mean - x_mean)/x_mean) *100

# Output array
item = unique(x$item) %>% droplevels()
avg = pcc
se = se_pcc

out = data.frame(item, avg, se)

return(out)

})

luz_delta <- luz_delta %>%
  mutate(pos = avg >= 0,
         SE_lower = avg-se,
         SE_upper = avg+se)

p_change_delta <- luz_delta %>%
  mutate(item=as.factor(item),
         item=fct_reorder(item, avg)) %>%
  ggplot( aes(x=avg, y=item, fill=pos)) +
  geom_bar(stat="identity") +
  geom_errorbar(aes(xmin=SE_lower, xmax=SE_upper, width=0.2)) 
  theme(legend.position = "none",
        panel.grid = element_blank(),
        axis.title = element_blank(),
        axis.text = element_blank(),
        panel.background = element_blank())
  

p_change_delta

svg("images/p_change_delta.svg", height=7, width=12)
plot(p_change_delta)
dev.off()

getwd()

```


# MDS plot


I've found that the MDS plot changes a lot depending on what variables I use to aggregate.  Here I've used all the variables from the model except asset_ID, as the unique assets created a lot of background noise.
```{r MDS-LGA-top15, warning=FALSE, message=FALSE, include=FALSE, results='hide'}
#find top 15 items
top15 <- df %>%
  filter(item!="Metal scrap") %>%
  group_by(item) %>%
  summarise(sum=sum(sum)) %>%
  arrange(desc(sum)) %>%
  top_n(15, sum)
  
#create vector from top 15 items
top15 <- top15$item 
top15 <- unlist(top15)


df_top15 <- df %>%
  filter(item %in% top15)

#create abundance long format
 abund_long <- df_top15 %>%
    group_by(LGA, LUZ, covid, item) %>% 
    summarise(sum=sum(sum)) %>% #total of each item type per survey event
  ungroup()

#convert to data wide
  abund <- spread(abund_long, item, sum) 

  
  com <- abund[4:18]
  
  #removing debris items that are zero across all surveys
  com <- com[, which(colSums(com) != 0)]
  
  ord <- metaMDS(com)
  
  ord$stress
  
  data.scores <- as.data.frame(scores(ord)) #create df of ord points
  data.scores$LUZ <- abund$LUZ
  data.scores$covid <- abund$covid
  data.scores <- data.scores %>%
    mutate(covid = as.factor(covid),
           LUZ = as.factor(LUZ))
  
  species.scores <- as.data.frame(scores(ord, "species"))
  species.scores$item <- rownames(species.scores) #create item id column
  
  
# #Hull data LUZ
# CBD <- data.scores[data.scores$LUZ == "CBD", ][chull(data.scores[data.scores$LUZ ==
#     "CBD", c("NMDS1", "NMDS2")]), ]  # hull values for CBD
# Transport <- data.scores[data.scores$LUZ == "Public Transport Terminal", ][chull(data.scores[data.scores$LUZ ==
#     "Public Transport Terminal", c("NMDS1", "NMDS2")]), ]  # hull values for Transport
# Indus <- data.scores[data.scores$LUZ == "Industrial Precinct", ][chull(data.scores[data.scores$LUZ == "Industrial Precinct", c("NMDS1", "NMDS2")]), ]  # hull values for Industry
# Shop <- data.scores[data.scores$LUZ == "Shopping Centre", ][chull(data.scores[data.scores$LUZ ==
#     "Shopping Centre", c("NMDS1", "NMDS2")]), ]  # hull values for shopping
# 
# #create polygons
# LUZ.hull.data <- rbind(CBD, Transport, Indus, Shop)  #combine groups
# LUZ.hull.data
# 
# #Hull data covid
# covid1 <- data.scores[data.scores$covid == "1", ][chull(data.scores[data.scores$covid ==
#     "1", c("NMDS1", "NMDS2")]), ]  # hull values for Pre-Covid
# covid2 <- data.scores[data.scores$covid == "2", ][chull(data.scores[data.scores$covid ==
#     "2", c("NMDS1", "NMDS2")]), ]  # hull values for During Covid
# 
# #create polygons
# cov.hull.data <- rbind(covid1,covid2)  #combine groups
# cov.hull.data

# New facet label names for covid variable
data.scores$covid <- factor(data.scores$covid, levels = c("1", "2"),
                  labels = c("Pre-Covid", "During Covid"))

#creating ellipses

#add group column to data.scores
NMDS <- data.frame(NMDS1 = ord$points[,1], NMDS2 = ord$points[,2],group=data.scores$LUZ)

 veganCovEllipse<-function (cov, center = c(0, 0), scale = 1, npoints = 100) 
  {
    theta <- (0:npoints) * 2 * pi/npoints
    Circle <- cbind(cos(theta), sin(theta))
    t(center + scale * t(Circle %*% chol(cov)))
  }

  df_ell <- data.frame()
  for(g in levels(NMDS$group)){
    df_ell <- rbind(df_ell, cbind(as.data.frame(with(NMDS[NMDS$group==g,],
                    veganCovEllipse(cov.wt(cbind(NMDS1,NMDS2),wt=rep(1/length(NMDS1),length(NMDS1)))$cov,center=c(mean(NMDS1),mean(NMDS2)))))
                    ,group=g))
    

  }
# #ellipses for covid  
#   #add group column to data.scores
# cov_NMDS <- data.frame(NMDS1 = ord$points[,1], NMDS2 = ord$points[,2],group=data.scores$covid)
# 
#  veganCovEllipse<-function (cov, center = c(0, 0), scale = 1, npoints = 100) 
#   {
#     theta <- (0:npoints) * 2 * pi/npoints
#     Circle <- cbind(cos(theta), sin(theta))
#     t(center + scale * t(Circle %*% chol(cov)))
#   }
# 
#   df_ell_cov <- data.frame()
#   for(g in levels(cov_NMDS$group)){
#     df_ell_cov <- rbind(df_ell_cov, cbind(as.data.frame(with(cov_NMDS[cov_NMDS$group==g,],
#                     veganCovEllipse(cov.wt(cbind(NMDS1,NMDS2),wt=rep(1/length(NMDS1),length(NMDS1)))$cov,center=c(mean(NMDS1),mean(NMDS2)))))
#                     ,group=g))
#   }
  
  #The first method is calculating the ellipse paths using standard deviation and no scaling. The second method is using standard error and is also scaling the data. Thus, the plot produced with the first method can also be achieved with the second method by making the necessary changes to the ordiellipse function (kind='sd', not 'se'), and removing the scale (ord[[g]]$scale) from the veganCovEllipse function.
  #https://stackoverflow.com/questions/42799838/follow-up-plotting-ordiellipse-function-from-vegan-package-onto-nmds-plot-creat
```

```{r MDS-plot, warning=FALSE, message=FALSE}
#MDS plot LUZ
p_mds <- ggplot() +
  geom_point(data=data.scores, 
             aes(x=NMDS1, y=NMDS2, colour=LUZ, shape=covid), alpha=0.8) + 
  geom_path(data=df_ell, aes(x=NMDS1, y=NMDS2,colour=group),
            size=1, linetype=2) +
  coord_equal() +
  theme_bw()

p_mds

svg("images/p_mds.svg", height=7, width=7)
plot(p_mds)
dev.off()

p_mds_txt <- ggplot() +
  geom_text(data=species.scores,aes(x=NMDS1,y=NMDS2, label=item)) +
  geom_point(data=data.scores, 
             aes(x=NMDS1, y=NMDS2, colour=LUZ, shape=covid), alpha=0.8) + 
  geom_path(data=df_ell, aes(x=NMDS1, y=NMDS2,colour=group), 
            size=1, linetype=2) +
  coord_equal() +
  theme_bw()

svg("images/p_mds_txt.svg", height=7, width=7)
plot(p_mds_txt)
dev.off()

p_mds
p_mds_txt

## MDS plot covid
# ggplot() +
#   geom_text(data=species.scores,aes(x=NMDS1,y=NMDS2, label=item)) +
#   geom_point(data=data.scores,
#              aes(x=NMDS1, y=NMDS2, colour=covid, shape=LUZ), alpha=0.8) +
#     geom_path(data=df_ell_cov, aes(x=NMDS1, y=NMDS2,colour=group), 
#             size=1, linetype=2) +
#   coord_equal() +
#   theme_bw()
```

# Testing community data


```{r adonis-pairwise, warning=FALSE, message=FALSE}

#Both land use and covid have significant scores, but there is not a significant interaction between them
df.adonis <- adonis(com ~ data.scores$LUZ*data.scores$covid, permutations=999, method="bray")
df.adonis

#testing land use dispersion
df.dis <- vegdist(com, method = "bray") # make a distance matrix
df.betadisper.luz <- betadisper(df.dis, data.scores$LUZ) # test dispersion
df.betadisper.luz
permutest(df.betadisper.luz) #dispersion not significant

#testing covid dispersion
df.dis <- vegdist(com, method = "bray") # make a distance matrix
df.betadisper.cov <- betadisper(df.dis, data.scores$covid) # test dispersion
df.betadisper.cov
permutest(df.betadisper.cov) #dispersion not significant

citation("pairwiseAdonis")
```

# Micro debris

```{r create-df, warning=FALSE, message=FALSE, results='hide', include=FALSE}
#create survey count  

        #load item counts
        micro <- read.csv(file="Data/200630_micro.csv", 
                         header=T, sep=",") 

        #remove duplicate entries
        micro <- unique(micro[!duplicated(micro),])
        
        micro$tot <- as.integer(micro$tot)
        
        micro <- micro %>%
        group_by(event_id, item) %>%
        summarise(sum=sum(tot)) %>%
        ungroup()
        
        #load events
        micro_event <- read.csv(file="Data/2101_event.csv", 
                         header=T, sep=",") 
        
        #join event info to item count
        micro_survey_count <- left_join(micro_event, micro, by="event_id")
        
        micro_survey_count <- micro_survey_count %>%
          dplyr::select(-event.tot, -event.wt, -vol, -hr, event_date)
        
#create abundance data with all possible debris items and zero values
      
      #create full list of possible items (only four micro categories)
     micro_item <- as.data.frame(unique(micro$item))
      
      
      #create a df of all possible items at all survey events, complete with zeros
      micro_abund <- full_join(micro_survey_count, 
                               micro_item, by=c("item" = "unique(micro$item)"))
      
      #data long to wide
      micro_abund_wide <- spread(micro_abund, item, sum)
      
      micro_abund_wide <- micro_abund_wide %>%
        dplyr::select(event_id, "Glass beads", "Secondary plastics", "Primary plastics", "Polystyrene balls")
      
      #replace na values with 0
      micro_abund_wide[is.na(micro_abund_wide)] <- 0
      
      #transform back to long but now with the full items list and zero data
      micro_abund_long <- melt(micro_abund_wide, id.vars="event_id")
     
      micro_df <- left_join(micro_abund_long, micro_event, by="event_id")
      
      #load sites
          micro_site <- read.csv(file="Data/200619_site.csv", 
                           header=T, sep=",") 
      
            #join LGA and luz to df
          micro_df <- left_join(micro_df, micro_site, by = "Asset.ID")
      
      
      micro_df <- micro_df %>%
        dplyr::select(-event_date, -event.tot, -event.wt, -vol, -hr, -Site, -Latitude, -Longitude) %>% #remove duplicate columns
        rename(item=variable, #rename columns
               sum=value,
               asset_id = Asset.ID,
               cycle=Cycle,
               LUZ = Land.use.zone) %>% 
        mutate(asset_id=as.factor(asset_id), #classify columns correctly  
             item=as.factor(item),
             LGA=as.factor(LGA),
             LUZ=as.factor(LUZ),
             event_id=as.factor(event_id),
             cycle=as.factor(cycle),
             date=dmy(date)
             )
      
      
#Combine C8 and C9    
     #isolate C8
    micro_C8 <- micro_df %>%
      filter(cycle == "C8")
    
    #isolate C9
    micro_C9 <- micro_df %>%
      filter(cycle == "C9")
    
    #join C8 and C9 by unique drain and item to capture double surveys
    micro_t <- left_join(micro_C9, micro_C8, by=c("asset_id"="asset_id", "item" = "item"))
    micro_t$sum.y[is.na(micro_t$sum.y)] <- 0 #change na to 0
    
    micro_t <- micro_t %>%
      mutate(combined_sum = sum.y + sum.x) %>% #add C8 and C9 together
      dplyr::select(event_id.x, item, combined_sum)
    
    micro_df <- micro_df %>% 
      left_join(t, by = c("event_id"="event_id.x", "item"="item")) %>%
      filter(cycle != "C8") %>%
      mutate(combined_sum = replace_na(combined_sum, 0),
              sum2 = ifelse(cycle != "C9", sum, 0),
              final_sum = sum2 + combined_sum) %>%
      dplyr::select(-sum2, -sum, -combined_sum) %>%
      rename(sum = final_sum)
    
        micro_t <- micro_df %>%
      group_by(LUZ) %>%
      summarise(total=sum(sum)) %>% #>90% in industrial
      mutate(pct = total/737651  * 100)
```

```{r micro-glmm, warning=FALSE, message=FALSE}
#aggregate data
micro_mod <- micro_df %>%
  group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
  summarise(sum=sum(sum)) %>%
  ungroup() %>%
  # filter(cycle %in% paste0("C",1:7)) %>%
  mutate(LGA = as.factor(LGA),
         LUZ = as.factor(fct_relevel(LUZ, "CBD", "Shopping Centre", "Public Transport Terminal", "Industrial Precinct")),
         covid = as.factor(covid),
         cycle = as.factor(cycle))
       
#create model
micro_m  <- glmmTMB(sum ~ #Debris count per survey event
                LUZ * covid + #predicted by land use effected by covid
                 (1|LGA/asset_id) + (1|cycle), #suburb random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = micro_mod)

emmeans(micro_m, pairwise~covid|LUZ, type="response")

#create predictor df
micro_nd <- expand.grid(covid = unique(micro_mod$covid),
                  LUZ = unique(micro_mod$LUZ),
                  LGA = NA,
                  asset_id = NA,
                  cycle = NA)

#predict mean values of response variable
micro_pred <- predict(object=micro_m,
                newdata=micro_nd,
                se.fit=T,
                re.form=NA)

#create standard error for graphing
micro_nd$Total <- exp(micro_pred$fit)
micro_nd$SE_upper <- exp(micro_pred$fit + micro_pred$se.fit)
micro_nd$SE_lower <- exp(micro_pred$fit - micro_pred$se.fit)

# New facet label names for covid variable
micro_nd$covid <- factor(micro_nd$covid, levels = c("1", "2"),
                  labels = c("Pre-Covid", "During Covid"))

#create plot
micro_p_glmm <- ggplot(micro_nd, aes(y=Total, x=LUZ, fill=covid)) + 
  geom_bar(position="dodge", stat="identity") +
  geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2), position=position_dodge(width=0.90)) +
  labs(title="Micro debris count (every 6 weeks)",
       subtitle="By land use zone and covid restriction",
       x="", y="Total debris items per asset", tag="") 

micro_p_glmm

#create plot
# micro_p_glmm <- ggplot(micro_nd, aes(y=Total, x=LUZ, fill=covid)) + 
#   geom_bar(position="dodge", stat="identity") +
#   geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2), position=position_dodge(width=0.90)) +
#   labs(title="Micro debris count (every 6 weeks)",
#        subtitle="By land use zone and covid restriction",
#        x="", y="Total debris items per asset", tag="") +
#       theme(legend.position = "none",
#         panel.grid = element_blank(),
#         axis.title = element_blank(),
#         axis.text = element_blank(),
#         panel.background = element_blank())
# 
# svg("images/micro_p_glmm.svg", height=7, width=12)
# plot(micro_p_glmm)
# dev.off()
```


```{r percent-change-delta-micro}
#refer to delta method
#https://stats.stackexchange.com/questions/64652/calculating-standard-deviation-associated-with-percentage-change
micro_pct_change <- micro_df %>%
  group_by(covid, asset_id, LUZ) %>% #look at items per asset both cov
  mutate(assetMean=mean(sum)) %>% #mean of item btwn 8 cycles at each asset per covid level
  ungroup() %>%
  dplyr::select(covid, asset_id, LUZ, assetMean) %>% #select vars
  unique() #remove duplicates from survey cycle
  
# Split data into each item
micro_pct_change <- droplevels(micro_pct_change)
micro_ls.data_split <- split(micro_pct_change, f = micro_pct_change$LUZ)


micro_delta_LUZ = purrr::map_df(micro_ls.data_split, function(x){

micro_x1 = x %>% filter(covid == 1) #x1 pre-covid

micro_x2 = x %>% filter(covid == 2) %>% dplyr::select(asset_id, assetMean) #x2 during covid

micro_xV = var(micro_x1$assetMean) #variance of pre-covid mean per asset
micro_x_mean = mean(micro_x1$assetMean) #mean of pre-covid mean per asset

micro_yV = var(micro_x2$assetMean) #variance of during covid mean per asset
micro_y_mean = mean(micro_x2$assetMean) #mean of during covid mean per asset

#referencing the second to last equation in the stack exchange answer linked above
micro_num = (micro_yV*micro_x_mean^2) - 2*cov(micro_x1$assetMean, micro_x2$assetMean) * micro_x_mean * micro_y_mean + (micro_xV*micro_y_mean^2) 

micro_denom = micro_x_mean^4

micro_se_pcc = 100*sqrt(micro_num/micro_denom) #se pct change 

micro_pcc = ((micro_y_mean - micro_x_mean)/micro_x_mean) *100 #mean pct change based on averaged asset means

# Output array
LUZ = unique(x$LUZ)
micro_avg = micro_pcc
micro_se = micro_se_pcc

micro_out = data.frame(LUZ, micro_avg, micro_se)


})

micro_delta_LUZ <- micro_delta_LUZ %>%
  mutate(pos = micro_avg >= 0,
         micro_SE_lower = micro_avg-micro_se,
         micro_SE_upper = micro_avg+micro_se)

micro_p_delta_LUZ <- micro_delta_LUZ %>%
  mutate(LUZ=as.factor(LUZ),
         LUZ=fct_reorder(LUZ, micro_avg)) %>%
  ggplot( aes(x=micro_avg, y=LUZ, fill=pos)) +
  geom_bar(stat="identity") +
  geom_errorbar(aes(xmin=micro_SE_lower, xmax=micro_SE_upper, width=0.2)) 
  # theme(legend.position = "none",
  #       panel.grid = element_blank(),
  #       axis.title = element_blank(),
  #       axis.text = element_blank(),
  #       panel.background = element_blank())

micro_p_delta_LUZ

# svg("images/p_change_delta.svg", height=7, width=12)
# plot(p_change_delta)
# dev.off()

```