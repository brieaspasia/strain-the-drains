---
title: "Results"
author: "Brie Sherow"
date: "22/03/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(ggplot2) #graphing
library(ggthemes) #graphing templates
library(hrbrthemes) #graphing templates
library(lubridate) #date manipulation
library(forcats) #working with factors
library(tidyverse) #manipulating data
library(knitr) #rmarkdown functions
# library(kableExtra) #table layouts
library(tidyr) #long & wide formats
library(reshape2) #melt function for wide and long formats
library(stats) #R stats functions
library(broom) #create summaries from stats objects
library(car) #lm regression
library(MASS) #stats
library(lme4) #glmer function
library(DHARMa) #testing model diagnostics
library(glmmTMB) #fit zero-inflated negative binomial
library(lattice) #fourth corner heatmap
library(corrplot) #co-occurrence matrix
library(gclus) #co-occurrence matrix
library(broom.mixed) #regression tables
library(vegan) #ordination
library(emmeans)
library(ComplexHeatmap)
library(RColorBrewer)
library(heatmaply)
library(cowplot) #minimal backgrounds for ggplot

```

```{r create-df}

#create survey count  

        #load item counts
        item <- read.csv(file="Data/2101_item.csv", 
                         header=T, sep=",") 

  #remove duplicate entries
        item <- unique(item[!duplicated(item),])

        #load item labels
        item_label <- read.csv(file="Data/item_label.csv", 
                         header=T, sep=",") 
        
        #load events
        event <- read.csv(file="Data/2101_event.csv", 
                         header=T, sep=",") 
        
        #join event info to item count
        survey_count <- left_join(item, event, by="event_id")
        
        survey_count <- survey_count %>%
          dplyr::select(-event.tot, -event.wt, -vol, -hr, -event_date, -note) %>%
          filter(item!="Pollution Rating") %>%
          mutate(event_item = paste(event_id, item, sep=" "))

        #summarise duplicate survey entries (this will lose one set of notes....)
        item_merge <- survey_count %>%
          group_by(event_item) %>%
          summarise(sum=sum(total)) %>%
         ungroup()

        #join survey count to the merged summaries
        survey_count <- item_merge %>%
          left_join(survey_count, by="event_item") %>%
          dplyr::select(-total) %>%
          unique()
        
#create abundance data with all possible debris items and zero values

      #count of items per unique event
      event_count <- survey_count %>%
        group_by(event_id, item) %>%
        summarise(sum=sum(sum)) %>%
        ungroup()

      #load AMDI code\
      AMDI_code <- read.csv(file="Data/200101_AMDI_code.csv", 
                       header=T, sep=",") 
      
      #create full list of possible items
      AMDI <- AMDI_code %>%
        filter(item_code_id != "LSTD1") %>% #remove pollution rating / microplastics
        dplyr::select(item.ex) #column of all possible item types
      
      
      #create a df of all possible items at all survey events, complete with zeros
      abund <- full_join(AMDI, event_count, by=c("item.ex" = "item"))
      
      #remove empty row
      abund <- abund[-11613, ]
      
      #data long to wide
      abund_wide <- spread(abund, item.ex, sum)
      
      #replace na values with 0
      abund_wide[is.na(abund_wide)] <- 0
        
      #transform back to long but now with the full items list and zero data
      abund_long <- melt(abund_wide, id.vars="event_id")
      
      #remove the zero event ID (how did that get there?)
      abund_long <- abund_long[abund_long$event_id != 0, ]
      
      #create event_item column for joining
      abund_long <- abund_long %>%
      mutate(event_item = paste(event_id, variable, sep=" "))

#join abundance data to survey notes and other columns
  df <- left_join(abund_long, survey_count, by="event_item") 
  
  df <- df %>%
    dplyr::select(-event_item, -event_id.y, -item, -sum) %>% #remove duplicate columns
    rename(event_id=event_id.x,
           item=variable,
           sum=value) %>% #rename columns
    arrange(event_id, Asset.ID) %>% #arrange by event so that missing values are aligned
    fill(Asset.ID, Cycle, date, covid) #fill in missing values to match survey data

  #clean up column classifications
  
  #reclassify columns to factor
    names <- c("event_id", "Asset.ID", "Cycle")
  
    df[,names] <- lapply(df[,names], factor)
  
  #reclassify date
    df$date <- dmy(df$date)

#join land use zone and LGA
    
    #load sites
        site <- read.csv(file="Data/200619_site.csv", 
                         header=T, sep=",") 
        
    #join LGA and luz to df
        df <- left_join(df, site, by = "Asset.ID")
        
    #remove unecessary columns
        df <- df %>%
          dplyr::select(-Site, -Latitude, -Longitude) 
     
#resolve duplicates in MC01, C5    
        dup <- df %>% 
          group_by(Asset.ID, Cycle, item) %>% #reconcile duplicates in MC01 Cycle 5
          summarise(sum=sum(sum)) %>%
          ungroup() %>%
          mutate(join = paste(Asset.ID, Cycle, item, sep=" "))
         
    # #check difference
    #      subsetdf <- df %>%
    #        dplyr::select(Asset.ID, Cycle, item, sum)
    #      
    #      setdiff(subsetdf, dup)
         
    #join C5 correction to df
         df <- df %>%
           mutate(join = paste(Asset.ID, Cycle, item, sep=" "))
         
    df <- dup %>% left_join(df, by="join") %>%
      dplyr::select(-join, -item.y, -sum.y, -Asset.ID.y, -Cycle.y) %>%
      filter(event_id != "23097") %>%
      rename(
          asset_id = Asset.ID.x, 
          cycle = Cycle.x,
          item = item.x,
          sum = sum.x,
          LUZ = Land.use.zone,
      ) %>%
      distinct()
    
    item_labelDF <- df %>%
      distinct(item)
    
    df <- df %>%
      left_join(item_label, by=c("item" = "item.type")) %>%
      mutate(item = A3) %>%
      dplyr::select(-A3)
    
      
 # lost_items <- anti_join(item_label, item_labelDF, by=c("item.type"="item"))   
    
  # df <- df %>%
  #   filter(cycle %in% paste0("C",1:7))
    df <- df %>%
      mutate(asset_id=as.factor(asset_id),
             item=as.factor(item),
             LGA=as.factor(LGA),
             LUZ=as.factor(LUZ)
             )
```

```{r combine-cycles-8-9}
#isolate C8
C8 <- df %>%
  filter(cycle == "C8")

#isolate C9
C9 <- df %>%
  filter(cycle == "C9")

#join C8 and C9 by unique drain and item to capture double surveys
t <- left_join(C9, C8, by=c("asset_id"="asset_id", "item" = "item"))
t$sum.y[is.na(t$sum.y)] <- 0 #change na to 0

t <- t %>%
  mutate(combined_sum = sum.y + sum.x) %>% #add C8 and C9 together
  dplyr::select(event_id.x, item, combined_sum)

df <- df %>% 
  left_join(t, by = c("event_id"="event_id.x", "item"="item")) %>%
  filter(cycle != "C8") %>%
  mutate(combined_sum = replace_na(combined_sum, 0),
          sum2 = ifelse(cycle != "C9", sum, 0),
          final_sum = sum2 + combined_sum) %>%
  dplyr::select(-sum2, -sum, -combined_sum) %>%
  rename(sum = final_sum)

```


```{r heatmap-item}

top20 <- df %>%
  filter(cycle %in% c("C1", "C2", "C3", "C4")) %>%
  group_by(item) %>%
  summarise(total=sum(sum)) %>%
  arrange(desc(total)) %>%
  top_n(20) %>%
  dplyr::select(item)

top20 <- as.vector(unlist(top20))

top_LUZ <- df %>%
  filter(cycle %in% c("C1", "C2", "C3", "C4"), item %in% top20) %>%
  group_by(LUZ) %>%
  mutate(total_LUZ=sum(sum)) %>%
  ungroup() %>%
  group_by(LUZ, item) %>%
  mutate(total_itemLUZ = sum(sum)) %>%
  ungroup() %>%
  mutate(pct = (total_itemLUZ/total_LUZ)*100) %>%
  dplyr::select(item, LUZ, pct) %>%
  unique() %>%
  spread(item, pct) %>%
  column_to_rownames(var="LUZ")

mat <- data.matrix(top_LUZ) #matrix for heatmap

mat2 = ifelse(mat <= 0.9,"<1",as.character(round(mat,2))) #matrix for labels

col_ha <- HeatmapAnnotation(Region = anno_text(colnames(mat), location = 1, rot = 60, 
                              just = "right"))

Heatmap(log10(mat), 
        name = "Debris count (percentage per land use zone)", 
        heatmap_legend_param  = list(color_bar = "continuous", at = c(-2.6,3.5),
                      title = "Debris counts (% total)",labels = c("Low", "High"),
                      grid_width = unit(0.8, "cm"),
                      legend_height = unit(5, "cm"),
                      title_position = "leftcenter-rot"), 
        col= colorRampPalette(brewer.pal(8, "PRGn"))(25),
        row_dend_reorder = TRUE, 
        row_order = sort(rownames(mat)),
        show_column_dend = FALSE,
        show_row_dend = FALSE,
        bottom_annotation = col_ha,
        show_row_names = TRUE,
        row_names_side = "left",
        show_column_names = FALSE,
        border = TRUE,
        column_title = "Percentage of debris items in each land use zone",
        #percentage label
        cell_fun = function(j, i, x, y, width, height, fill) {
        grid.text(sprintf("%s", mat2[i, j]), x, y, gp = gpar(fontsize = 10))},
        )


# heatmaply(mat, xlab="Land Use Zones", ylab="Debris Items", show_dendrogram=F)
```
```{r MDS-pre-covid}

#find top 20 items
top20 <- df %>%
  filter(cycle %in% c("C1", "C2", "C3", "C4")) %>%
  group_by(item) %>%
  summarise(sum=sum(sum)) %>%
  arrange(desc(sum)) %>%
  top_n(20, sum)
  
#create vector from top 20 items
top20 <- top20$item 
top20 <- unlist(top20)


df_top20 <- df %>%
  filter(cycle %in% c("C1", "C2", "C3", "C4")) %>%
  filter(item %in% top20)

#create abundance long format
 abund_long <- df_top20 %>%
    group_by(asset_id, LUZ, item) %>% 
    summarise(sum=sum(sum)) %>% #total of each item type per survey event
  ungroup()

#convert to data wide
  abund <- spread(abund_long, item, sum) 

  
  com <- abund[3:22]
  
  #removing debris items that are zero across all surveys
  com <- com[, which(colSums(com) != 0)]
  
  ord <- metaMDS(com)
  
  data.scores <- as.data.frame(scores(ord)) #create df of ord points
  data.scores$LUZ <- abund$LUZ
  data.scores <- data.scores %>%
    mutate(LUZ = as.factor(LUZ))
  
  species.scores <- as.data.frame(scores(ord, "species"))
  species.scores$item <- rownames(species.scores) #create item id column
  
  
#Hull data
CBD <- data.scores[data.scores$LUZ == "CBD", ][chull(data.scores[data.scores$LUZ ==
    "CBD", c("NMDS1", "NMDS2")]), ]  # hull values for CBD
Transport <- data.scores[data.scores$LUZ == "Public Transport Terminal", ][chull(data.scores[data.scores$LUZ ==
    "Public Transport Terminal", c("NMDS1", "NMDS2")]), ]  # hull values for Transport
Indus <- data.scores[data.scores$LUZ == "Industrial Precinct", ][chull(data.scores[data.scores$LUZ == "Industrial Precinct", c("NMDS1", "NMDS2")]), ]  # hull values for Industry
Shop <- data.scores[data.scores$LUZ == "Shopping Centre", ][chull(data.scores[data.scores$LUZ ==
    "Shopping Centre", c("NMDS1", "NMDS2")]), ]  # hull values for shopping

#create polygons
hull.data <- rbind(CBD, Transport, Indus, Shop)  #combine groups
hull.data

  
ggplot() +
  geom_polygon(data=hull.data,aes(x=NMDS1,y=NMDS2,fill=LUZ,group=LUZ),alpha=0.1) +
  geom_text(data=species.scores,aes(x=NMDS1,y=NMDS2, label=item)) +
  geom_point(data=data.scores, 
             aes(x=NMDS1, y=NMDS2, colour=LUZ, shape=LUZ), alpha=0.8) + 
  coord_equal() +
  theme_bw()

#https://chrischizinski.github.io/rstats/vegan-ggplot2/

#colour symbols to land use and change shape to covid
#arrows (vectors that show how items are pulling/pushing)
```


```{r avg-items-per-lockdown}

items <- df %>%
  group_by(item) %>%
  summarise(total = sum(sum)) %>%
  arrange(desc(total)) %>%
  top_n(25)

item_per_cycle <- df %>%
  left_join(items, by="item") %>%
  filter(total > 205) %>%
  group_by(date, covid, LUZ, item) %>%
  summarise(cycle_sum = sum(sum))

covid_lvl <- data.frame(name = c("1", "2", "3", "4", "5"),
                   start = as.Date(c("2019-10-29", "2020-03-14", "2020-03-19", "2020-06-21",
                             "2020-07-06")),
                   end = as.Date(c("2020-03-14", "2020-03-19", "2020-06-21", "2020-07-06", 
                           "2020-10-17")),
                   level = c("No lockdown", "Moderate lockdown", "Strict lockdown", "Moderate lockdown", "Strict lockdown"),
                   stringsAsFactors = FALSE) %>%
  mutate(median_x = start + floor((end-start)/2))


p <- ggplot() +
  geom_rect(data=covid_lvl, aes(NULL, NULL, xmin=start, xmax=end, fill=level, 
                                ymin= -Inf, ymax=Inf), alpha=0.1) +
  labs(fill="Covid-19 lockdown", linetype="Land Use Zones", colour="Land Use Zones",
       x="Date", y="Debris count per survey cycle") +
  geom_line(data=item_per_cycle, aes(date, cycle_sum, linetype=LUZ, color=LUZ)) +
  theme(axis.text.x=element_text(angle=45, vjust=0.75, hjust=0.8)) +
  facet_wrap(~item, scales="free") 
  # theme_cowplot(12)
p

#https://plotly.com/ggplot2/geom_rect/ tutorial for geom_rect background
```

##GLMM and predictions
```{r data-mod}
mod <- df %>%
  group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
  summarise(sum=sum(sum)) %>%
  ungroup() %>%
  filter(cycle %in% c("C1", "C2", "C3", "C4", "C5", "C6", "C7")) %>%
  mutate(LGA = as.factor(LGA),
         LUZ = as.factor(LUZ),
         covid = as.factor(covid),
         cycle = as.factor(cycle))

# ggplot(data=mod, aes(x=covid, y=sum, color=LUZ)) +
#   geom_boxplot() +
#   scale_y_log10() 
       
```

```{r create-model}
m  <- glmmTMB(sum ~ #Debris count per survey event
                LUZ * covid + #predicted by land use effected by covid
                 (1|LGA/asset_id) + (1|cycle), #suburb random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod)

drop1(m, test="Chisq")

str(mod)

emmeans(m, ~covid|LUZ)
emmeans(m, pairwise~covid|LUZ, type="response")

```

```{r dharma-effect}
simulationOutput <- simulateResiduals(fittedModel = m, plot=T)
  
plotResiduals(simulationOutput)
testUniformity(simulationOutput) #tests if the overall distribution conforms to expectations
# testOutliers(simulationOutput) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput) #tests if the simulated dispersion is equal to the observed dispersion
#testQuantiles(simulationOutput) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
testZeroInflation(simulationOutput) #tests if there are more zeros than expected
```
```{r temporal-autocorrelation}
mod$resid = resid(m) #create a column for each survey event's residuals

mod_temp <- mod %>%
    group_by(date) %>%
    summarise(sum=sum(resid))

testTemporalAutocorrelation(mod_temp$sum, 
                              time =  mod_temp$date) #test resid for temporal autocorrelation

citation("DHARMa")
```

```{r predictions}
#create a new dataframe with columns of all fixed predictor variables in th emodel, and ros of all possible values of those predictor varables that you want predictions from.  Predictors are covid and LUZ.
nd <- expand.grid(covid = unique(mod$covid),
                  LUZ = unique(mod$LUZ),
                  LGA = NA,
                  asset_id = NA,
                  cycle = NA)

#predict function uses the model and the new df to predict mean values of the response variable.  It predicts a value for each row in the new df.
pred <- predict(object=m,
                newdata=nd,
                se.fit=T,
                re.form=NA,
                type="response")

#creating standard error for graphing
nd$Total <- pred$fit
nd$SE_upper <- pred$fit + pred$se.fit
nd$SE_lower <- pred$fit - pred$se.fit

#creating plot
p <- ggplot(nd, aes(y=Total, x=covid)) + 
  geom_col(aes(fill=covid)) +
  geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2)) +
  labs(title="Debris abundance predictions",
       subtitle="By land use zone and covid restriction",
       x="Covid lockdown status", y="Total debris items", tag="") +
  facet_wrap(~LUZ) +
  theme_minimal()

p

#group them by LUZ stacked bars

#also create a line graph (borrow from the one already made of the top items, covid lockdown shaded polygon in the background)

```
```{r data-mod-top5}
# Function loop: each item

top5_txt<- df %>%
  filter(item %in% c("Cigarette butts", 
                     "Plastic packaging (food)",
                     "Plastic wrap (non-food)",
                     "Soft plastic remnants",
                     "Paper stubs")) %>% 
  distinct(item)%>%
  pull(item) %>% 
  droplevels()
 
# Purrr
library(purrr)

pred_df = purrr::map_df(top5_txt, function(x){
  
  temp_df = df %>% 
    filter(item == paste0(x)) %>%
    group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
    summarise(sum=sum(sum)) %>%
    ungroup() %>%
    filter(cycle %in% paste0("C",1:7)) %>%
    mutate(LGA = as.factor(LGA),
           LUZ = as.factor(fct_relevel(LUZ, "CBD", "Shopping Centre", "Public Transport Terminal", "Industrial Precinct")),
           covid = as.factor(covid),
           cycle = droplevels(as.factor(cycle)),
           item = x)
  
 # p1 = ggplot(data=temp_df, aes(x=covid, y=sum+0.5, color=LUZ)) +
 #    geom_boxplot() +
 #    scale_y_log10() 
 
 #create model
 m_temp  <- glmmTMB(sum ~ #Debris count per survey event
                   LUZ * covid + #predicted by land use effected by covid
                   (1|LGA/asset_id) + (1|cycle), #suburb random effect
                   family=nbinom2(), #negative binomial to deal with count data and zeros
                   data = temp_df)

#  Model validation -------------------------------------------------------

 
 # emmeans(m_temp, ~covid|LUZ)
 # emmeans(m_temp, pairwise~covid|LUZ, type="response")
 
 #dharma cigarette
 # simulationOutput_cig <- simulateResiduals(fittedModel = m_cig, plot=T)
 # 
 # plotResiduals(simulationOutput_cig)
 # testUniformity(simulationOutput_cig) #tests if the overall distribution conforms to expectations
 # # testOutliers(simulationOutput_cig) #tests if there are more simulation outliers than expected
 # testDispersion(simulationOutput_cig) #tests if the simulated dispersion is equal to the observed dispersion
 # #testQuantiles(simulationOutput_cig) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
 # testZeroInflation(simulationOutput_cig) #tests if there are more zeros than expected


# Model validation end ----------------------------------------------------

 nd_temp <- expand.grid(covid = unique(temp_df$covid),
                       LUZ = unique(temp_df$LUZ),
                       LGA = NA,
                       asset_id = NA,
                       cycle = NA)
 
 #predict function uses the model and the new df to predict mean values of the response variable.  It predicts a value for each row in the new df.
 pred_temp <- predict(object=m_temp,
                     newdata=nd_temp,
                     se.fit=T,
                     re.form=NA,
                     type="response")
 
 #creating standard error for graphing
 nd_temp$Total <- pred_temp$fit
 nd_temp$SE_upper <- pred_temp$fit + pred_temp$se.fit
 nd_temp$SE_lower <- pred_temp$fit - pred_temp$se.fit
 nd_temp$item = x
 
 return(nd_temp)
 
 })


 #creating plot

 p_all <- ggplot(pred_df, aes(y=Total, x=LUZ, fill=covid)) + 
   geom_bar(stat="identity", position="dodge", show.legend=F) +
   geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2), position=position_dodge(width=0.90)) +
   labs(title="Top debris type abundance predictions",
        subtitle="By land use zone and covid restriction",
        x="Covid lockdown status", y="Total debris", tag="") +
   facet_wrap(~item) +
   theme_minimal()
 

 p_all
 
```

```{r data-mod-h&s}

#modify data
mod_hs <- df %>%
  filter(item=="OH & S") %>%
  group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
  summarise(sum=sum(sum)) %>%
  ungroup() %>%
  mutate(LGA = as.factor(LGA),
         LUZ = as.factor(LUZ),
         covid = as.factor(covid),
         cycle = as.factor(cycle))

ggplot(data=mod_hs, aes(x=covid, y=sum, color=LUZ)) +
  geom_boxplot()

#create model
m_hs  <- glmmTMB(sum ~ #Debris count per survey event
                LUZ * covid + #predicted by land use effected by covid
                 (1|LGA/asset_id) + (1|cycle), #suburb random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod_hs)

drop1(m_hs, test="Chisq")

str(mod_hs)

emmeans(m_hs, ~covid|LUZ)
emmeans(m_hs, pairwise~covid|LUZ, type="response")

#dharma health and safety
simulationOutput_hs <- simulateResiduals(fittedModel = m_hs, plot=T)
  
plotResiduals(simulationOutput_hs)
testUniformity(simulationOutput_hs) #tests if the overall distribution conforms to expectations
# testOutliers(simulationOutput_hs) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput_hs) #tests if the simulated dispersion is equal to the observed dispersion
#testQuantiles(simulationOutput_hs) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
testZeroInflation(simulationOutput_hs) #tests if there are more zeros than expected
```

```{r predictions-hs}
#create a new dataframe with columns of all fixed predictor variables in th emodel, and ros of all possible values of those predictor varables that you want predictions from.  Predictors are covid and LUZ.
nd_hs <- expand.grid(covid = unique(mod_hs$covid),
                  LUZ = unique(mod_hs$LUZ),
                  LGA = NA,
                  asset_id = NA,
                  cycle = NA)

#predict function uses the model and the new df to predict mean values of the response variable.  It predicts a value for each row in the new df.
pred_hs <- predict(object=m_hs,
                newdata=nd_hs,
                se.fit=T,
                re.form=NA,
                type="response")

#creating standard error for graphing
nd_hs$Total <- pred_hs$fit
nd_hs$SE_upper <- pred_hs$fit + pred_hs$se.fit
nd_hs$SE_lower <- pred_hs$fit - pred_hs$se.fit

#creating plot
p_hs <- ggplot(nd_hs, aes(y=Total, x=covid)) + 
  geom_col(aes(fill=covid)) +
  geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2)) +
  labs(title="Occupational health item abundance predictions",
       subtitle="By land use zone and covid restriction",
       x="Covid lockdown status", y="Total items", tag="") +
  facet_wrap(~LUZ) +
  theme_minimal()

p_hs

#group them by LUZ stacked bars

#also create a line graph (borrow from the one already made of the top items, covid lockdown shaded polygon in the background)

```
##MDS plots

```{r MDS-LGA-top20}
#find top 20 items
top20 <- df %>%
  group_by(item) %>%
  summarise(sum=sum(sum)) %>%
  arrange(desc(sum)) %>%
  top_n(20, sum)
  
#create vector from top 20 items
top20 <- top20$item 
top20 <- unlist(top20)


df_top20 <- df %>%
  filter(item %in% top20)

#create abundance long format
 abund_long <- df_top20 %>%
    group_by(LGA, LUZ, covid, item) %>% 
    summarise(sum=sum(sum)) %>% #total of each item type per survey event
  ungroup()

#convert to data wide
  abund <- spread(abund_long, item, sum) 

  
  com <- abund[4:23]
  
  #removing debris items that are zero across all surveys
  com <- com[, which(colSums(com) != 0)]
  
  ord <- metaMDS(com)
  
  data.scores <- as.data.frame(scores(ord)) #create df of ord points
  data.scores$LUZ <- abund$LUZ
  data.scores$covid <- abund$covid
  data.scores <- data.scores %>%
    mutate(covid = as.factor(covid),
           LUZ = as.factor(LUZ))
  
  species.scores <- as.data.frame(scores(ord, "species"))
  species.scores$item <- rownames(species.scores) #create item id column
  
  
#Hull data
CBD <- data.scores[data.scores$LUZ == "CBD", ][chull(data.scores[data.scores$LUZ ==
    "CBD", c("NMDS1", "NMDS2")]), ]  # hull values for CBD
Transport <- data.scores[data.scores$LUZ == "Public Transport Terminal", ][chull(data.scores[data.scores$LUZ ==
    "Public Transport Terminal", c("NMDS1", "NMDS2")]), ]  # hull values for Transport
Indus <- data.scores[data.scores$LUZ == "Industrial Precinct", ][chull(data.scores[data.scores$LUZ == "Industrial Precinct", c("NMDS1", "NMDS2")]), ]  # hull values for Industry
Shop <- data.scores[data.scores$LUZ == "Shopping Centre", ][chull(data.scores[data.scores$LUZ ==
    "Shopping Centre", c("NMDS1", "NMDS2")]), ]  # hull values for shopping

#create polygons
hull.data <- rbind(CBD, Transport, Indus, Shop)  #combine groups
hull.data

  
ggplot() +
  geom_polygon(data=hull.data,aes(x=NMDS1,y=NMDS2,fill=LUZ,group=LUZ),alpha=0.1) +
  geom_text(data=species.scores,aes(x=NMDS1,y=NMDS2, label=item)) +
  geom_point(data=data.scores, 
             aes(x=NMDS1, y=NMDS2, colour=LUZ, shape=covid), alpha=0.8) + 
  coord_equal() +
  theme_bw()

#https://chrischizinski.github.io/rstats/vegan-ggplot2/

#colour symbols to land use and change shape to covid
#arrows (vectors that show how items are pulling/pushing)
```
```{r MDS-asset-top20}
#find top 20 items
top20 <- df %>%
  group_by(item) %>%
  summarise(sum=sum(sum)) %>%
  arrange(desc(sum)) %>%
  top_n(20, sum)
  
#create vector from top 20 items
top20 <- top20$item 
top20 <- unlist(top20)


df_top20 <- df %>%
  filter(item %in% top20)

#create abundance long format
 abund_long <- df_top20 %>%
    group_by(asset_id, LUZ, covid, item) %>% 
    summarise(sum=sum(sum)) %>% #total of each item type per survey event
  ungroup()

#convert to data wide
  abund <- spread(abund_long, item, sum) 

  
  com <- abund[4:23]
  
  #removing debris items that are zero across all surveys
  com <- com[, which(colSums(com) != 0)]
  
  ord <- metaMDS(com)
  
  data.scores <- as.data.frame(scores(ord)) #create df of ord points
  data.scores$LUZ <- abund$LUZ
  data.scores$covid <- abund$covid
  data.scores <- data.scores %>%
    mutate(covid = as.factor(covid),
           LUZ = as.factor(LUZ))
  
  species.scores <- as.data.frame(scores(ord, "species"))
  species.scores$item <- rownames(species.scores) #create item id column
  
  
#Hull data
CBD <- data.scores[data.scores$LUZ == "CBD", ][chull(data.scores[data.scores$LUZ ==
    "CBD", c("NMDS1", "NMDS2")]), ]  # hull values for CBD
Transport <- data.scores[data.scores$LUZ == "Public Transport Terminal", ][chull(data.scores[data.scores$LUZ ==
    "Public Transport Terminal", c("NMDS1", "NMDS2")]), ]  # hull values for Transport
Indus <- data.scores[data.scores$LUZ == "Industrial Precinct", ][chull(data.scores[data.scores$LUZ == "Industrial Precinct", c("NMDS1", "NMDS2")]), ]  # hull values for Industry
Shop <- data.scores[data.scores$LUZ == "Shopping Centre", ][chull(data.scores[data.scores$LUZ ==
    "Shopping Centre", c("NMDS1", "NMDS2")]), ]  # hull values for shopping

#create polygons
hull.data <- rbind(CBD, Transport, Indus, Shop)  #combine groups
hull.data

  
ggplot() +
  geom_polygon(data=hull.data,aes(x=NMDS1,y=NMDS2,fill=LUZ,group=LUZ),alpha=0.1) +
  geom_text(data=species.scores,aes(x=NMDS1,y=NMDS2, label=item)) +
  geom_point(data=data.scores, 
             aes(x=NMDS1, y=NMDS2, colour=LUZ, shape=covid), alpha=0.8) + 
  coord_equal() +
  theme_bw()

#https://chrischizinski.github.io/rstats/vegan-ggplot2/

#colour symbols to land use and change shape to covid
#arrows (vectors that show how items are pulling/pushing)
```



```{r adonis-pairwise}


# #Rosie's example
# rockpools.adonis <- adonis(RockPools_vars_std ~ RockPools$Habitat*RockPools$Year,permutations = 999, method = "bray")
# rockpools.adonis

df.adonis <- adonis(com ~ data.scores$LUZ*data.scores$covid, permutations=999, method="bray")
df.adonis

#testing land use dispersion
df.dis <- vegdist(com, method = "bray") # make a distance matrix
df.betadisper.luz <- betadisper(df.dis, data.scores$LUZ) # test dispersion
df.betadisper.luz
permutest(df.betadisper.luz) #dispersion not significant

#testing covid dispersion
df.dis <- vegdist(com, method = "bray") # make a distance matrix
df.betadisper.cov <- betadisper(df.dis, data.scores$covid) # test dispersion
df.betadisper.cov
permutest(df.betadisper.cov) #dispersion not significant

#pairwise land use comparisons -- differences in CBD vs Industrial and Transport vs Industrial
LUZ.pair <- pairwise.adonis(df.dis, abund$LUZ, sim.method="bray", perm = 999, sim.function = "vegdist")
LUZ.pair

#pairwise land use comparisons -- differences between all pairs
cov.pair <- pairwise.adonis(df.dis, abund$covid, sim.method="bray", perm = 999, sim.function = "vegdist")
cov.pair

#different whether aggregating between LGA or asset???

```