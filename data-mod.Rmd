---
title: "drains-data-mod"
author: "Brie Sherow"
date: "06/02/2021"
output: html_document
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    df_print: paged
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
getwd()
```

```{r libraries}
library(ggplot2) #graphing
library(ggthemes) #graphing templates
library(hrbrthemes) #graphing templates
library(lubridate) #date manipulation
library(forcats) #working with factors
library(tidyverse) #manipulating data
library(knitr) #rmarkdown functions
library(kableExtra) #table layouts
library(tidyr) #long & wide formats
library(reshape2) #melt function for wide and long formats
library(stats) #R stats functions
library(broom) #create summaries from stats objects
library(car) #lm regression
library(MASS) #stats
library(lme4) #glmer function
library(DHARMa) #testing model diagnostics
library(glmmTMB) #fit zero-inflated negative binomial
library(lattice) #fourth corner heatmap
library(corrplot) #co-occurrence matrix
library(gclus) #co-occurrence matrix
library(broom.mixed) #regression tables
library(vegan) #ordination
library(emmeans)
```

```{r create-df}

#create survey count  

        #load item counts
        item <- read.csv(file="Data/2101_item.csv", 
                         header=T, sep=",") 
        
        #load events
        event <- read.csv(file="Data/2101_event.csv", 
                         header=T, sep=",") 
        
        #join event info to item count
        survey_count <- left_join(item, event, by="event_id")
        
        survey_count <- survey_count %>%
          dplyr::select(-event.tot, -event.wt, -vol, -hr, -note) %>%
          filter(item!="Pollution Rating") %>%
          mutate(event_item = paste(event_id, item, sep=" "))

        #summarise duplicate survey entries (this will lose one set of notes....)
        item_merge <- survey_count %>%
          group_by(event_item) %>%
          summarise(sum=sum(total)) %>%
         ungroup()

        #join survey count to the merged summaries
        survey_count <- item_merge %>%
          left_join(survey_count, by="event_item") %>%
          dplyr::select(-total) %>%
          unique()
        
#create abundance data with all possible debris items and zero values

      #count of items per unique event
      event_count <- survey_count %>%
        group_by(event_id, item) %>%
        summarise(sum=sum(sum)) %>%
        ungroup()

      #load AMDI code\
      AMDI_code <- read.csv(file="Data/200101_AMDI_code.csv", 
                       header=T, sep=",") 
      
      #create full list of possible items
      AMDI <- AMDI_code %>%
        filter(item_code_id != "LSTD1") %>% #remove pollution rating / microplastics
        dplyr::select(item.ex) #column of all possible item types
      
      
      #create a df of all possible items at all survey events, complete with zeros
      abund <- full_join(AMDI, event_count, by=c("item.ex" = "item"))
      
      #remove empty row
      abund <- abund[-11613, ]
      
      #data long to wide
      abund_wide <- spread(abund, item.ex, sum)
      
      #replace na values with 0
      abund_wide[is.na(abund_wide)] <- 0
        
      #transform back to long but now with the full items list and zero data
      abund_long <- melt(abund_wide, id.vars="event_id")
      
      #remove the zero event ID (how did that get there?)
      abund_long <- abund_long[abund_long$event_id != 0, ]
      
      #create event_item column for joining
      abund_long <- abund_long %>%
      mutate(event_item = paste(event_id, variable, sep=" "))

#join abundance data to survey notes and other columns
  df <- left_join(abund_long, survey_count, by="event_item") 
  
  df <- df %>%
    dplyr::select(-event_item, -event_id.y, -item, -sum) %>% #remove duplicate columns
    rename(event_id=event_id.x,
           item=variable,
           sum=value) %>% #rename columns
    arrange(event_id, Asset.ID) %>% #arrange by event so that missing values are aligned
    fill(Asset.ID, Cycle, date, covid) #fill in missing values to match survey data

  #clean up column classifications
  
  #reclassify columns to factor
    names <- c("event_id", "Asset.ID", "Cycle")
  
    df[,names] <- lapply(df[,names], factor)
  
  #reclassify date
    df$date <- dmy(df$date)

#join land use zone and LGA
    
    #load sites
        site <- read.csv(file="Data/200619_site.csv", 
                         header=T, sep=",") 
        
    #join LGA and luz to df
        df <- left_join(df, site, by = "Asset.ID")
        
    #remove unecessary columns
        df <- df %>%
          dplyr::select(-Site, -Latitude, -Longitude) 
     
#resolve duplicates in MC01, C5    
        dup <- df %>% 
          group_by(Asset.ID, Cycle, item) %>% #reconcile duplicates in MC01 Cycle 5
          summarise(sum=sum(sum)) %>%
          ungroup() %>%
          mutate(join = paste(Asset.ID, Cycle, item, sep=" "))
         
    # #check difference
    #      subsetdf <- df %>%
    #        dplyr::select(Asset.ID, Cycle, item, sum)
    #      
    #      setdiff(subsetdf, dup)
         
    #join C5 correction to df
         df <- df %>%
           mutate(join = paste(Asset.ID, Cycle, item, sep=" "))
         
    df <- dup %>% left_join(df, by="join") %>%
      dplyr::select(-join, -item.y, -sum.y, -Asset.ID.y, -Cycle.y) %>%
      filter(event_id != "23097") %>%
      rename(
          asset_id = Asset.ID.x, 
          cycle = Cycle.x,
          item = item.x,
          sum = sum.x,
          LUZ = Land.use.zone,
      ) %>%
      distinct()
        
```


```{r day-diff}

#find days since last survey at each asset
diff <- df %>%
  distinct(event_id, .keep_all = T) %>%
  dplyr::select(-item,-sum) %>%
  group_by(asset_id) %>% #looking at each event
  arrange(date) %>%
  mutate(days = date - lag(date)) %>% #days since last survey event
  filter(days != "NA") %>% #remove first survey date
  ungroup()

diff_red <- diff %>% dplyr::select(event_id, asset_id, days)

df <- df %>%
  left_join(diff_red, by="event_id") %>%
  dplyr::select(-asset_id.y) %>%
  rename(asset_id = asset_id.x)

#calculate diff since last event, then factor in the sum per item

str(df)

df$days <- as.numeric(df$days)

t <- df %>%
  mutate(change = (sum/days))

# https://rdrr.io/cran/DataCombine/man/PercChange.html
```


```{r data-mod}
mod <- df %>%
  group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
  summarise(sum=sum(sum)) %>%
  ungroup() %>%
  filter(cycle %in% c("C1", "C2", "C3", "C4", "C5", "C6", "C7")) %>%
  mutate(LGA = as.factor(LGA),
         LUZ = as.factor(LUZ),
         covid = as.factor(covid),
         cycle = as.factor(cycle))

ggplot(data=mod, aes(x=covid, y=sum, color=LUZ)) +
  geom_boxplot() +
  scale_y_log10() 
       
```
```{r top-items}
#2 Round totals and top items
    
    df_sum <- df %>%
  group_by(item, cycle, covid) %>% #relevant variables
  summarise(sum=sum(sum), avg=mean(sum), sd=sd(sum)) %>%
  ungroup() %>%
  filter(cycle %in% c("C1", "C2", "C3", "C4", "C5", "C6", "C7")) %>%
  mutate(covid = as.factor(covid),
         cycle = as.factor(cycle))
    
    # labs <- c("Oct 2019","Dec 2019","Jan 2020","Mar 2020", "Apr 2020", "May 2020", "July 2020")
    # levels(df_sum$cycle) <- labs
    # 
    # unique(df_sum$cycle) #why are the levels still 9 after it's been filtered to 7?
    
    top_items <- df_sum %>%
      group_by(cycle) %>%
      top_n(10, sum) 

#add a covid lockdown polygon behind this graph
  line_item <- ggplot(top_items, aes(x = cycle, y = sum, group = item, colour=item)) +
      geom_line() +
      geom_point() +
      ggtitle("Item count per Round") +
      guides(colour=guide_legend("Item type")) +
      ylab("Total item count")
    line_item
```


```{r create-model}
m  <- glmmTMB(sum ~ #Debris count per survey event
                LUZ * covid + #predicted by land use effected by covid
                 (1|LGA/asset_id) + (1|cycle), #suburb random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod)

drop1(m, test="Chisq")

str(mod)

emmeans(m, ~covid|LUZ)
emmeans(m, pairwise~covid|LUZ, type="response")
```


```{r dharma-effect}
simulationOutput <- simulateResiduals(fittedModel = m, plot=T)
  
plotResiduals(simulationOutput)
testUniformity(simulationOutput) #tests if the overall distribution conforms to expectations
# testOutliers(simulationOutput) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput) #tests if the simulated dispersion is equal to the observed dispersion
#testQuantiles(simulationOutput) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
testZeroInflation(simulationOutput) #tests if there are more zeros than expected
```
```{r temporal-autocorrelation}
mod$resid = resid(m) #create a column for each survey event's residuals

mod_temp <- mod %>%
    group_by(date) %>%
    summarise(sum=sum(resid))

testTemporalAutocorrelation(mod_temp$sum, 
                              time =  mod_temp$date) #test resid for temporal autocorrelation
```

```{r predictions}
#create a new dataframe with columns of all fixed predictor variables in th emodel, and ros of all possible values of those predictor varables that you want predictions from.  Predictors are covid and LUZ.
nd <- expand.grid(covid = unique(mod$covid),
                  LUZ = unique(mod$LUZ),
                  LGA = NA,
                  asset_id = NA,
                  cycle = NA)

#predict function uses the model and the new df to predict mean values of the response variable.  It predicts a value for each row in the new df.
pred <- predict(object=m,
                newdata=nd,
                se.fit=T,
                re.form=NA,
                type="response")

#creating standard error for graphing
nd$Total <- pred$fit
nd$SE_upper <- pred$fit + pred$se.fit
nd$SE_lower <- pred$fit - pred$se.fit

#creating plot
p <- ggplot(nd, aes(y=Total, x=covid)) + 
  geom_col(aes(fill=covid)) +
  geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2)) +
  labs(title="Debris abundance predictions",
       subtitle="By land use zone and covid restriction",
       x="Covid lockdown status", y="Total debris items", tag="") +
  facet_wrap(~LUZ) +
  theme_minimal()

p

#group them by LUZ stacked bars

#also create a line graph (borrow from the one already made of the top items, covid lockdown shaded polygon in the background)

```
```{r vegan}
#find top 20 items
top20 <- df %>%
  group_by(item) %>%
  summarise(sum=sum(sum)) %>%
  arrange(desc(sum)) %>%
  top_n(20, sum)
  
#create vector from top 20 items
top20 <- top20$item 
top20 <- unlist(top20)


df_top20 <- df %>%
  filter(item %in% top20)

#create abundance long format
 abund_long <- df_top20 %>%
    group_by(event_id, LUZ, item) %>% 
    summarise(sum=sum(sum)) %>% #total of each item type per survey event
  ungroup()

#convert to data wide
  abund <- spread(abund_long, item, sum) 

  #convert survey event to rowname  
  length(unique(abund$event_id)) == nrow(abund) #checking for duplicates
  abund <- column_to_rownames(abund, var="event_id") #create rownames from survey event
  
  com <- abund[2:21]
  
  #removing debris items that are zero across all surveys
  com <- com[, which(colSums(com) != 0)]

  ord <- metaMDS(com)
  plot(ord)
  
plot(ord, type = "n")
points(ord, display = "sites", cex = 0.8, pch=21, col="yellow")
text(ord, display = "spec", cex=0.7, col="blue")
```
```{r MDS}
#create a dissimilarity matrix that measures similarity between every pair of samples
abund.mds <- metaMDS(comm = com, distance = "bray", trace = FALSE, autotransform = FALSE)

#create mds ordination plot
plot(abund.mds$points)

#extract x-y coords to new df
MDS_xy <- data.frame(abund.mds$points)

#add habitat and location to these coordinates
MDS_xy$LUZ <- abund$LUZ

MDS_ec <- ggplot(MDS_xy, aes(MDS1, MDS2, color = LUZ)) + geom_point() + theme_bw()
MDS_ec
```

