---
title: "drains-data-mod"
author: "Brie Sherow"
date: "06/02/2021"
output: html_document
  html_document:
    toc: yes
    toc_float: yes
    code_folding: hide
    df_print: paged
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
getwd()
```

```{r libraries}
library(ggplot2) #graphing
library(ggthemes) #graphing templates
library(hrbrthemes) #graphing templates
library(lubridate) #date manipulation
library(forcats) #working with factors
library(tidyverse) #manipulating data
library(knitr) #rmarkdown functions
library(kableExtra) #table layouts
library(tidyr) #long & wide formats
library(reshape2) #melt function for wide and long formats
library(stats) #R stats functions
library(broom) #create summaries from stats objects
library(car) #lm regression
library(MASS) #stats
library(lme4) #glmer function
library(DHARMa) #testing model diagnostics
library(glmmTMB) #fit zero-inflated negative binomial
library(lattice) #fourth corner heatmap
library(corrplot) #co-occurrence matrix
library(gclus) #co-occurrence matrix
library(broom.mixed) #regression tables
library(vegan) #ordination
library(emmeans)
```

```{r create-df}

#create survey count  

        #load item counts
        item <- read.csv(file="Data/2101_item.csv", 
                         header=T, sep=",") 
        
        #load events
        event <- read.csv(file="Data/2101_event.csv", 
                         header=T, sep=",") 
        
        #join event info to item count
        survey_count <- left_join(item, event, by="event_id")
        
        survey_count <- survey_count %>%
          dplyr::select(-event.tot, -event.wt, -vol, -hr, -note) %>%
          filter(item!="Pollution Rating") %>%
          mutate(event_item = paste(event_id, item, sep=" "))

        #summarise duplicate survey entries (this will lose one set of notes....)
        item_merge <- survey_count %>%
          group_by(event_item) %>%
          summarise(sum=sum(total)) %>%
         ungroup()

        #join survey count to the merged summaries
        survey_count <- item_merge %>%
          left_join(survey_count, by="event_item") %>%
          dplyr::select(-total) %>%
          unique()
        
#create abundance data with all possible debris items and zero values

      #count of items per unique event
      event_count <- survey_count %>%
        group_by(event_id, item) %>%
        summarise(sum=sum(sum)) %>%
        ungroup()

      #load AMDI code\
      AMDI_code <- read.csv(file="Data/200101_AMDI_code.csv", 
                       header=T, sep=",") 
      
      #create full list of possible items
      AMDI <- AMDI_code %>%
        filter(item_code_id != "LSTD1") %>% #remove pollution rating / microplastics
        dplyr::select(item.ex) #column of all possible item types
      
      
      #create a df of all possible items at all survey events, complete with zeros
      abund <- full_join(AMDI, event_count, by=c("item.ex" = "item"))
      
      #remove empty row
      abund <- abund[-11613, ]
      
      #data long to wide
      abund_wide <- spread(abund, item.ex, sum)
      
      #replace na values with 0
      abund_wide[is.na(abund_wide)] <- 0
        
      #transform back to long but now with the full items list and zero data
      abund_long <- melt(abund_wide, id.vars="event_id")
      
      #remove the zero event ID (how did that get there?)
      abund_long <- abund_long[abund_long$event_id != 0, ]
      
      #create event_item column for joining
      abund_long <- abund_long %>%
      mutate(event_item = paste(event_id, variable, sep=" "))

#join abundance data to survey notes and other columns
  df <- left_join(abund_long, survey_count, by="event_item") 
  
  df <- df %>%
    dplyr::select(-event_item, -event_id.y, -item, -sum) %>% #remove duplicate columns
    rename(event_id=event_id.x,
           item=variable,
           sum=value) %>% #rename columns
    arrange(event_id, Asset.ID) %>% #arrange by event so that missing values are aligned
    fill(Asset.ID, Cycle, date, covid) #fill in missing values to match survey data

  #clean up column classifications
  
  #reclassify columns to factor
    names <- c("event_id", "Asset.ID", "Cycle")
  
    df[,names] <- lapply(df[,names], factor)
  
  #reclassify date
    df$date <- dmy(df$date)

#join land use zone and LGA
    
    #load sites
        site <- read.csv(file="Data/200619_site.csv", 
                         header=T, sep=",") 
        
    #join LGA and luz to df
        df <- left_join(df, site, by = "Asset.ID")
        
    #remove unecessary columns
        df <- df %>%
          dplyr::select(-Site, -Latitude, -Longitude) 
     
#resolve duplicates in MC01, C5    
        dup <- df %>% 
          group_by(Asset.ID, Cycle, item) %>% #reconcile duplicates in MC01 Cycle 5
          summarise(sum=sum(sum)) %>%
          ungroup() %>%
          mutate(join = paste(Asset.ID, Cycle, item, sep=" "))
         
    # #check difference
    #      subsetdf <- df %>%
    #        dplyr::select(Asset.ID, Cycle, item, sum)
    #      
    #      setdiff(subsetdf, dup)
         
    #join C5 correction to df
         df <- df %>%
           mutate(join = paste(Asset.ID, Cycle, item, sep=" "))
         
    df <- dup %>% left_join(df, by="join") %>%
      dplyr::select(-join, -item.y, -sum.y, -Asset.ID.y, -Cycle.y) %>%
      filter(event_id != "23097") %>%
      rename(
          asset_id = Asset.ID.x, 
          cycle = Cycle.x,
          item = item.x,
          sum = sum.x,
          LUZ = Land.use.zone,
      ) %>%
      distinct()
        
```

```{r combine-cycles-8-9}

C8 <- df %>%
  filter(cycle == "C8") %>%
  mutate(asset_item = paste(asset_id, item))

C9 <- df %>%
  filter(cycle == "C9") %>%
  mutate(asset_item = paste(asset_id, item))

t <- left_join(C9, C8, by="asset_item")
t$sum.y[is.na(t$sum.y)] <- 0

t <- t %>%
  mutate(combined_sum = sum.y + sum.x)

C8 <- t2 %>%
  dplyr::select(asset_id.x, item.x, combined_sum, date.x, date.y, covid.x, LGA.x, LUZ.x) %>%
  mutate(date=paste0(date.y, ", ", date.x)) %>%
  rename(asset_id = asset_id.x,
         item = item.x,
         sum = combined_sum,
         covid=covid.x,
         LGA=LGA.x,
         LUZ=LUZ.x) %>%
  dplyr::select(-date.x, -date.y)

#replace C8 and 9 with new C8
  df <- df %>%
  filter(cycle %in% paste0("C",1:7)) 
```

```{r data-mod}
mod <- df %>%
  group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
  summarise(sum=sum(sum)) %>%
  ungroup() %>%
  # filter(cycle %in% c("C1", "C2", "C3", "C4", "C5", "C6", "C7")) %>%
  mutate(LGA = as.factor(LGA),
         LUZ = as.factor(LUZ),
         covid = as.factor(covid),
         cycle = as.factor(cycle))

ggplot(data=mod, aes(x=covid, y=sum, color=LUZ)) +
  geom_boxplot() +
  scale_y_log10() 
       
```

```{r create-model}
m  <- glmmTMB(sum ~ #Debris count per survey event
                LUZ * covid + #predicted by land use effected by covid
                 (1|LGA/asset_id) + (1|cycle), #suburb random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod)

drop1(m, test="Chisq")

str(mod)

emmeans(m, ~covid|LUZ)
emmeans(m, pairwise~covid|LUZ, type="response")
```

```{r dharma-effect}
simulationOutput <- simulateResiduals(fittedModel = m, plot=T)
  
plotResiduals(simulationOutput)
testUniformity(simulationOutput) #tests if the overall distribution conforms to expectations
# testOutliers(simulationOutput) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput) #tests if the simulated dispersion is equal to the observed dispersion
#testQuantiles(simulationOutput) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
testZeroInflation(simulationOutput) #tests if there are more zeros than expected
```
```{r temporal-autocorrelation}
mod$resid = resid(m) #create a column for each survey event's residuals

mod_temp <- mod %>%
    group_by(date) %>%
    summarise(sum=sum(resid))

testTemporalAutocorrelation(mod_temp$sum, 
                              time =  mod_temp$date) #test resid for temporal autocorrelation
```

```{r predictions}
#create a new dataframe with columns of all fixed predictor variables in th emodel, and ros of all possible values of those predictor varables that you want predictions from.  Predictors are covid and LUZ.
nd <- expand.grid(covid = unique(mod$covid),
                  LUZ = unique(mod$LUZ),
                  LGA = NA,
                  asset_id = NA,
                  cycle = NA)

#predict function uses the model and the new df to predict mean values of the response variable.  It predicts a value for each row in the new df.
pred <- predict(object=m,
                newdata=nd,
                se.fit=T,
                re.form=NA,
                type="response")

#creating standard error for graphing
nd$Total <- pred$fit
nd$SE_upper <- pred$fit + pred$se.fit
nd$SE_lower <- pred$fit - pred$se.fit

#creating plot
p <- ggplot(nd, aes(y=Total, x=covid)) + 
  geom_col(aes(fill=covid)) +
  geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2)) +
  labs(title="Debris abundance predictions",
       subtitle="By land use zone and covid restriction",
       x="Covid lockdown status", y="Total debris items", tag="") +
  facet_wrap(~LUZ) +
  theme_minimal()

p

#group them by LUZ stacked bars

#also create a line graph (borrow from the one already made of the top items, covid lockdown shaded polygon in the background)

```
```{r data-mod-top5}
# Function loop: each item

top5_txt<- df %>%
  filter(item %in% c("Cigarette butts & filters", 
                     "Plastic packaging food (wrap, packets, containers)",
                     "Plastic wrap non food (bubble wrap etc)",
                     "Plastic film remnants (bits of plastic bag, wrap etc)",
                     "Miscellaneous paper, labels & tickets")) %>% 
  distinct(item)%>%
  pull(item) %>% 
  droplevels()
 
# Purrr
library(purrr)

pred_df = purrr::map_df(top5_txt, function(x){
  
  temp_df = df %>% 
    filter(item == paste0(x)) %>%
    group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
    summarise(sum=sum(sum)) %>%
    ungroup() %>%
    filter(cycle %in% paste0("C",1:7)) %>%
    mutate(LGA = as.factor(LGA),
           LUZ = as.factor(fct_relevel(LUZ, "CBD", "Shopping Centre", "Public Transport Terminal", "Industrial Precinct")),
           covid = as.factor(covid),
           cycle = droplevels(as.factor(cycle)),
           item = x)
  
 # p1 = ggplot(data=temp_df, aes(x=covid, y=sum+0.5, color=LUZ)) +
 #    geom_boxplot() +
 #    scale_y_log10() 
 
 #create model
 m_temp  <- glmmTMB(sum ~ #Debris count per survey event
                   LUZ * covid + #predicted by land use effected by covid
                   (1|LGA/asset_id) + (1|cycle), #suburb random effect
                   family=nbinom2(), #negative binomial to deal with count data and zeros
                   data = temp_df)

#  Model validation -------------------------------------------------------

 
 # emmeans(m_temp, ~covid|LUZ)
 # emmeans(m_temp, pairwise~covid|LUZ, type="response")
 
 #dharma cigarette
 # simulationOutput_cig <- simulateResiduals(fittedModel = m_cig, plot=T)
 # 
 # plotResiduals(simulationOutput_cig)
 # testUniformity(simulationOutput_cig) #tests if the overall distribution conforms to expectations
 # # testOutliers(simulationOutput_cig) #tests if there are more simulation outliers than expected
 # testDispersion(simulationOutput_cig) #tests if the simulated dispersion is equal to the observed dispersion
 # #testQuantiles(simulationOutput_cig) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
 # testZeroInflation(simulationOutput_cig) #tests if there are more zeros than expected


# Model validation end ----------------------------------------------------

 nd_temp <- expand.grid(covid = unique(temp_df$covid),
                       LUZ = unique(temp_df$LUZ),
                       LGA = NA,
                       asset_id = NA,
                       cycle = NA)
 
 #predict function uses the model and the new df to predict mean values of the response variable.  It predicts a value for each row in the new df.
 pred_temp <- predict(object=m_temp,
                     newdata=nd_temp,
                     se.fit=T,
                     re.form=NA,
                     type="response")
 
 #creating standard error for graphing
 nd_temp$Total <- pred_temp$fit
 nd_temp$SE_upper <- pred_temp$fit + pred_temp$se.fit
 nd_temp$SE_lower <- pred_temp$fit - pred_temp$se.fit
 nd_temp$item = x
 
 return(nd_temp)
 
 })


 #creating plot

 p_all <- ggplot(pred_df, aes(y=Total, x=LUZ)) + 
   geom_bar(stat="identity", position="dodge", aes(fill=covid)) +
   geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2, 
                     stat="identity", position="dodge")) +
   labs(title="Top debris type abundance predictions",
        subtitle="By land use zone and covid restriction",
        x="Covid lockdown status", y="Total debris", tag="") +
   facet_wrap(~item) +
   theme_minimal()
 

 p_all
 
```

```{r data-mod-cig}

#modify data
mod_cig <- df %>%
  filter(item=="Cigarette butts & filters") %>%
  group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
  summarise(sum=sum(sum)) %>%
  ungroup() %>%
  filter(cycle %in% c("C1", "C2", "C3", "C4", "C5", "C6", "C7")) %>%
  mutate(LGA = as.factor(LGA),
         LUZ = as.factor(LUZ),
         covid = as.factor(covid),
         cycle = as.factor(cycle))

ggplot(data=mod_cig, aes(x=covid, y=sum, color=LUZ)) +
  geom_boxplot() +
  scale_y_log10() 

#create model
m_cig  <- glmmTMB(sum ~ #Debris count per survey event
                LUZ * covid + #predicted by land use effected by covid
                 (1|LGA/asset_id) + (1|cycle), #suburb random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod_cig)

drop1(m_cig, test="Chisq")

str(mod_cig)

emmeans(m_cig, ~covid|LUZ)
emmeans(m_cig, pairwise~covid|LUZ, type="response")

#dharma cigarette
simulationOutput_cig <- simulateResiduals(fittedModel = m_cig, plot=T)
  
plotResiduals(simulationOutput_cig)
testUniformity(simulationOutput_cig) #tests if the overall distribution conforms to expectations
# testOutliers(simulationOutput_cig) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput_cig) #tests if the simulated dispersion is equal to the observed dispersion
#testQuantiles(simulationOutput_cig) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
testZeroInflation(simulationOutput_cig) #tests if there are more zeros than expected
```

```{r predictions-cig}
#create a new dataframe with columns of all fixed predictor variables in th emodel, and ros of all possible values of those predictor varables that you want predictions from.  Predictors are covid and LUZ.
nd_cig <- expand.grid(covid = unique(mod_cig$covid),
                  LUZ = unique(mod_cig$LUZ),
                  LGA = NA,
                  asset_id = NA,
                  cycle = NA)

#predict function uses the model and the new df to predict mean values of the response variable.  It predicts a value for each row in the new df.
pred_cig <- predict(object=m_cig,
                newdata=nd_cig,
                se.fit=T,
                re.form=NA,
                type="response")

#creating standard error for graphing
nd_cig$Total <- pred_cig$fit
nd_cig$SE_upper <- pred_cig$fit + pred_cig$se.fit
nd_cig$SE_lower <- pred_cig$fit - pred_cig$se.fit

#creating plot
p_cig <- ggplot(nd_cig, aes(y=Total, x=covid)) + 
  geom_col(aes(fill=covid)) +
  geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2)) +
  labs(title="Cigarette abundance predictions",
       subtitle="By land use zone and covid restriction",
       x="Covid lockdown status", y="Total cigarettes", tag="") +
  facet_wrap(~LUZ) +
  theme_minimal()

p_cig

#group them by LUZ stacked bars

#also create a line graph (borrow from the one already made of the top items, covid lockdown shaded polygon in the background)

```

```{r data-mod-plas-food}

#modify data
mod_pf <- df %>%
  filter(item=="Plastic packaging food (wrap, packets, containers)") %>%
  group_by(event_id, asset_id, cycle, date, covid, LGA, LUZ) %>% #relevant variables
  summarise(sum=sum(sum)) %>%
  ungroup() %>%
  filter(cycle %in% c("C1", "C2", "C3", "C4", "C5", "C6", "C7")) %>%
  mutate(LGA = as.factor(LGA),
         LUZ = as.factor(LUZ),
         covid = as.factor(covid),
         cycle = as.factor(cycle))

ggplot(data=mod_pf, aes(x=covid, y=sum, color=LUZ)) +
  geom_boxplot() +
  scale_y_log10() 

#create model
m_pf  <- glmmTMB(sum ~ #Debris count per survey event
                LUZ * covid + #predicted by land use effected by covid
                 (1|LGA/asset_id) + (1|cycle), #suburb random effect
               family=nbinom2(), #negative binomial to deal with count data and zeros
               data = mod_pf)

drop1(m_pf, test="Chisq")

str(mod_pf)

emmeans(m_pf, ~covid|LUZ)
emmeans(m_pf, pairwise~covid|LUZ, type="response")

#dharma cigarette
simulationOutput_pf <- simulateResiduals(fittedModel = m_pf, plot=T)
  
plotResiduals(simulationOutput_pf)
testUniformity(simulationOutput_pf) #tests if the overall distribution conforms to expectations
# testOutliers(simulationOutput_pf) #tests if there are more simulation outliers than expected
testDispersion(simulationOutput_pf) #tests if the simulated dispersion is equal to the observed dispersion
#testQuantiles(simulationOutput_pf) #fits a quantile regression or residuals against a predictor (default predicted value), and tests of this conforms to the expected quantile
testZeroInflation(simulationOutput_pf) #tests if there are more zeros than expected
```

```{r predictions-plas-food}
#create a new dataframe with columns of all fixed predictor variables in th emodel, and ros of all possible values of those predictor varables that you want predictions from.  Predictors are covid and LUZ.
nd_pf <- expand.grid(covid = unique(mod_pf$covid),
                  LUZ = unique(mod_pf$LUZ),
                  LGA = NA,
                  asset_id = NA,
                  cycle = NA)

#predict function uses the model and the new df to predict mean values of the response variable.  It predicts a value for each row in the new df.
pred_pf <- predict(object=m_pf,
                newdata=nd_pf,
                se.fit=T,
                re.form=NA,
                type="response")

#creating standard error for graphing
nd_pf$Total <- pred_pf$fit
nd_pf$SE_upper <- pred_pf$fit + pred_pf$se.fit
nd_pf$SE_lower <- pred_pf$fit - pred_pf$se.fit

#creating plot
p_pf <- ggplot(nd_pf, aes(y=Total, x=covid)) + 
  geom_col(aes(fill=covid)) +
  geom_errorbar(aes(ymin=SE_lower, ymax=SE_upper, width=0.2)) +
  labs(title="Plastic food wrap abundance predictions",
       subtitle="By land use zone and covid restriction",
       x="Covid lockdown status", y="Total plastic food wrap", tag="") +
  facet_wrap(~LUZ) +
  theme_minimal()

p_pf

#group them by LUZ stacked bars

#also create a line graph (borrow from the one already made of the top items, covid lockdown shaded polygon in the background)

```




```{r vegan-top5}
#find top 5 items
top5 <- df %>%
  group_by(item) %>%
  summarise(sum=sum(sum)) %>%
  arrange(desc(sum)) %>%
  top_n(5, sum)
  
#create vector from top 20 items
top5 <- top5$item 
top5 <- unlist(top5)


df_top5 <- df %>%
  filter(item %in% top5)

#create abundance long format
 abund_long <- df_top5 %>%
    group_by(event_id, LUZ, covid, item) %>% 
    summarise(sum=sum(sum)) %>% #total of each item type per survey event
  ungroup()

#convert to data wide
  abund <- spread(abund_long, item, sum) 

  #convert survey event to rowname  
  length(unique(abund$event_id)) == nrow(abund) #checking for duplicates
  abund <- column_to_rownames(abund, var="event_id") #create rownames from survey event
  
  com <- abund[3:7]
  
  #removing debris items that are zero across all surveys
  com <- com[, which(colSums(com) != 0)]
  
#create df to join covid and LUZ to ord
  
  df_env <- df %>%
    dplyr::select("event_id", "covid", "LUZ") %>%
    distinct()
    
  
  ord <- metaMDS(com)
  plot(ord)
  
  data.scores <- as.data.frame(scores(ord)) #create df of ord points
  data.scores$event_id <- rownames(data.scores) #create event id column
  data.scores <- left_join(data.scores, df_env, by="event_id")  #join LUZ, covid
  data.scores <- data.scores %>%
    mutate(covid = as.factor(covid),
           LUZ = as.factor(LUZ))
  
  species.scores <- as.data.frame(scores(ord, "species"))
  species.scores$item <- rownames(species.scores) #create item id column
  
  # abund2 = bind_cols(abund, mds1 = scores(ord)[,1],mds2 = scores(ord)[,2])
  
  
ggplot() +
  geom_polygon(data=hull.data,aes(x=NMDS1,y=NMDS2,fill=covid,group=covid),alpha=0.30) +
  geom_text(data=species.scores,aes(x=NMDS1,y=NMDS2, label=item)) +
  geom_point(data=data.scores, aes(x=NMDS1, y=NMDS2, colour=covid, shape=LUZ)) + 
  coord_equal() +
  theme_bw()

#https://chrischizinski.github.io/rstats/vegan-ggplot2/

#colour symbols to land use and change shape to covid
#arrows (vectors that show how items are pulling/pushing)
```

```{r vegan-top20}
#find top 20 items
top20 <- df %>%
  group_by(item) %>%
  summarise(sum=sum(sum)) %>%
  arrange(desc(sum)) %>%
  top_n(20, sum)
  
#create vector from top 20 items
top20 <- top20$item 
top20 <- unlist(top20)


df_top20 <- df %>%
  filter(item %in% top20)

#create abundance long format
 abund_long <- df_top20 %>%
    group_by(event_id, LUZ, covid, item) %>% 
    summarise(sum=sum(sum)) %>% #total of each item type per survey event
  ungroup()

#convert to data wide
  abund <- spread(abund_long, item, sum) 

  #convert survey event to rowname  
  length(unique(abund$event_id)) == nrow(abund) #checking for duplicates
  abund <- column_to_rownames(abund, var="event_id") #create rownames from survey event
  
  com <- abund[3:22]
  
  #removing debris items that are zero across all surveys
  com <- com[, which(colSums(com) != 0)]
  
#create df to join covid and LUZ to ord
  
  df_env <- df %>%
    dplyr::select("event_id", "covid", "LUZ") %>%
    distinct()
    
  
  ord <- metaMDS(com)
  
  data.scores <- as.data.frame(scores(ord)) #create df of ord points
  data.scores$event_id <- rownames(data.scores) #create event id column
  data.scores <- left_join(data.scores, df_env, by="event_id")  #join LUZ, covid
  data.scores <- data.scores %>%
    mutate(covid = as.factor(covid),
           LUZ = as.factor(LUZ))
  
  species.scores <- as.data.frame(scores(ord, "species"))
  species.scores$item <- rownames(species.scores) #create item id column
  
  # abund2 = bind_cols(abund, mds1 = scores(ord)[,1],mds2 = scores(ord)[,2])
  
  
ggplot() +
  geom_polygon(data=hull.data,aes(x=NMDS1,y=NMDS2,fill=covid,group=covid),alpha=0.30) +
  geom_text(data=species.scores,aes(x=NMDS1,y=NMDS2, label=item)) +
  geom_point(data=data.scores, aes(x=NMDS1, y=NMDS2, colour=covid, shape=LUZ)) + 
  coord_equal() +
  theme_bw()

#https://chrischizinski.github.io/rstats/vegan-ggplot2/

#colour symbols to land use and change shape to covid
#arrows (vectors that show how items are pulling/pushing)
```
```{r mds-hull}
covid1 <- data.scores[data.scores$covid == "1", ][chull(data.scores[data.scores$covid ==
    "1", c("NMDS1", "NMDS2")]), ]  # hull values for covid 1
covid2 <- data.scores[data.scores$covid == "2", ][chull(data.scores[data.scores$covid ==
    "2", c("NMDS1", "NMDS2")]), ]  # hull values for covid 2
covid3 <- data.scores[data.scores$covid == "3", ][chull(data.scores[data.scores$covid ==
    "3", c("NMDS1", "NMDS2")]), ]  # hull values for covid 3

hull.data <- rbind(covid1, covid2, covid3)  #combine grp.a and grp.b
hull.data
```


```{r vegan-subset}
#colour symbols to land use and change shape to covid
#arrows (vectors that show how items are pulling/pushing)

#join covid and land use to ORD$point (MDS x and y values)

  #isolate event_id as a column to join land use and covid level to ord
  points <- ord$points
  event_id <- rownames(points)
  points <- cbind(event_id, points)
  points <- as.data.frame(points)
  
  ord$points
  
  #create joining df with land use and covid level
  df_simple <- df %>%
    dplyr::select(event_id, LUZ, covid) %>%
    distinct()
  
  #join 
  points <- left_join(points, df_simple, by="event_id")
  
ord$points <- points  #this doesn't work because it is a different data str within ord

#create a separate layer to plot for each LUZ in each covid lockdown level

```

```{r MDS}
#create a dissimilarity matrix that measures similarity between every pair of samples
abund.mds <- metaMDS(comm = com, distance = "bray", trace = FALSE, autotransform = FALSE)

#create mds ordination plot
plot(abund.mds$points)

#extract x-y coords to new df
MDS_xy <- data.frame(abund.mds$points)

#add habitat and location to these coordinates
MDS_xy$LUZ <- abund$LUZ

MDS_ec <- ggplot(MDS_xy, aes(MDS1, MDS2, color = LUZ)) + geom_point() + theme_bw()
MDS_ec
```

